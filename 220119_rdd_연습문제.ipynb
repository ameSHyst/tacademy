{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e2282b",
   "metadata": {},
   "source": [
    "    1. k.txt 에 있는 각 단어의\n",
    "    갯수를 구하시요.\n",
    "\n",
    "    2. births.txt 파일을 이용하여 구하시요\n",
    "\n",
    "    1) 남아수의 합과 평균을 구하시요\n",
    "    2) 여아수의 합과 평균을 구하시요\n",
    "    3) 남아수가 높은 탑 5를 구하시요\n",
    "    4) 전체(남아여아의총수)중 남아수의 비율을 구하시요.\n",
    "    5) 여아수를 10등분 했을때 각계급의 수를 구하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8ea9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389d548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7356544d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 문희상 국회의장과 여야 3당 교섭단체 대표가 18일 국회정상화를 위해 머리를 맞댔지만 의미있는 결론을 내리지 못했다. ',\n",
       " '',\n",
       " '문 의장과 이인영 더불어민주당, 나경원 자유한국당, 오신환 바른미래당 원내대표는 이날 오후 국회의장실에서 회동을 갖고 국회정상화 방안을 논의했으나 의사일정을 잡는데 실패했다. ',\n",
       " '',\n",
       " '문 의장은 이 자리에서 여야 3당 원내대표에게 조속한 시일 내에 국회정상화에 합의할 것을 요청한 것으로 알려졌다. ',\n",
       " '',\n",
       " '전날 한국당을 제외한 바른미래당, 민주평화당, 정의당 등 야3당과 민주당 소속 의원들을 합해 98명이 6월 임시국회 소집요구서를 제출, 6월 임시국회가 반쪽으로 출발하게 됐지만, 한국당이 끝내 국회로 복귀하지 않을 경우 6월 임시국회도 난항이 예상된다. ',\n",
       " '',\n",
       " '더욱이 추가경정예산안을 비롯해 윤석열 검찰총장 후보자 인사청문회, 김현준 국세청장 후보자 인사청문회등 굵직한 일정이 남아있지만 한국당의 협조 없이는 이들 일정이 제대로 진행되기 어려워 일각에서는 임시국회만 소집해놓고 허송세월 하는게 아니냐 하는 우려의 목소리가 나온다. ',\n",
       " '',\n",
       " '이인영 민주당 원내대표는 회동 직후 기자들과 만나 \\'의사일정에 합의를 했느냐\\'는 기자들의 질문에 \"안됐다\"면서 \"의장님께서 합의하라고 말씀을 하셨는데 이견이 아직 해소될 상황이 아니다\"고 전했다. ',\n",
       " '',\n",
       " '이어 다음 회동을 묻는 질문에도 \"아직은 모르겠다\"고 말했다. ',\n",
       " '',\n",
       " '나경원 한국당 원내대표도 기자들과 만나 \\'회동에서 요청한 게 있느냐\\'는 질문에 \"오늘은 특별히 말씀드린 게 없는 것 같다\"고 답했다. ',\n",
       " '',\n",
       " '특히 나 원내대표는 \\'기획재정위원회에서 김현준 국세청장 후보자에 대한 인사청문회 개최에 합의를 한 것이 국회정상화에 응하겠다는 이야기냐\\'는 질문에 \"최종적으로 한 것은 아닌데 일부 소통에 문제가 있었던 것 같다\"며 \"청문회 부분은 적극적으로 들여다보고 있다\"고 말했다. ',\n",
       " '',\n",
       " '그는 \\'소통의 문제가 있다는 것은 국세청장 청문회가 확정되지 않았다는 것이냐\\'는 질문에 \"네. 저도 조금 더 확인해보겠다\"고 했다. ',\n",
       " '',\n",
       " '앞서 국회 기재위 여당 간사인 김정우 민주당 의원이 자유한국당을 포함한 여야 3당 간사가 합의에 김 후보자에 대한 인사청문회를 오는 26일 실시하기로 합의했다고 밝혔지만 나 원내대표에 따르면 이 일정도 확정이 안된 것으로 보인다. ',\n",
       " '',\n",
       " '다만 나 원내대표는 \\'국회 소집요구가 20일인데 그 전에 의사일정에 합의할 것이냐\\'는 질문에 \"국회를 정상국회로 만들고, 국회가 정말 대한민국의 미래, 경제와 민생을 살리는 국회가 되기 위해 계속 노력하겠다\"고 전했다. ',\n",
       " '',\n",
       " '',\n",
       " '문희상 국회의장과 여야 교섭단체 원내대표들이 18일 오후 서울 여의도 국회에서 회동을 갖고 있다. 왼쪽부터 이인영 더불어민주당 원내대표, 문 의장, 나경원 자유한국당, 오신환 바른미래당 원내대표. 2019.6.18/뉴스1 © News1 임세영 기자',\n",
       " '',\n",
       " '오신환 바른미래당 원내대표는 기자들과 만나 \"집권여당은 국정운영에 무한 책임을 지고 있다. 그 부분에 대해 민주당이 제대로 된 국회정상화를 위해 대승적 결단을 내려주길 바라고, 한국당은 조건없는 국회정상화에 참여하기를 바라는 마음\"이라고 밝혔다. ',\n",
       " '',\n",
       " '오 원내대표는 \\'여당의 대승적 결단이라고 함은 경제청문회를 받아줘야 한다는 것이냐\\'는 질문에 \"집권여당으로서 일부 포용하고 양보하는 것은 숙명이라고 생각한다\"며 \"야당이 경제문제에 대해 문제제기를 하는 것은 당연한 게 아니냐. 청문회건 어떤 형태가 되건 국회에서 정부를 상대로 할 수 있는 이야기 자체를 차단하고 못하게 하는 것은 올지 않다고 생각한다\"고 말했다. ',\n",
       " '',\n",
       " '이어 한국당이 위원장인 상임위의 경우 여당이 사회권을 발동하는 것에 바른미래당의 동참 여부를 묻는 질문에 \"그런 행동 자체가 무책임하다고 본다\"며 \"그것이 과연 국회를 정상적으로 운영하기 위한 마음을 갖고 있는 것인지, 장기적으로 추경을 처리하고 상임위 내에서 법안을 심사하고 국민을 위한 국회활동이 될 것인지에 대한 판단을 민주당이 해야 한다고 생각한다\"고 전했다. ',\n",
       " '',\n",
       " '오 원내대표는 윤석열 검찰총장 후보자 인사청문회와 관련 \"인사청문회는 당연히 절차에 따라 해야 하는 것\"이라며 \"윤 후보자가 적임자인지 여부는 국회에서 국민의 눈높이에 맞게 합리적인 인사청문 절차를 밟아가는 것이 마땅하다고 생각한다\"고 말했다.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kRdd = sc.textFile('data/k.txt')\n",
    "kRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf71ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'': 33,\n",
       "             '문희상': 2,\n",
       "             '국회의장과': 2,\n",
       "             '여야': 4,\n",
       "             '3당': 3,\n",
       "             '교섭단체': 2,\n",
       "             '대표가': 1,\n",
       "             '18일': 2,\n",
       "             '국회정상화를': 2,\n",
       "             '위해': 3,\n",
       "             '머리를': 1,\n",
       "             '맞댔지만': 1,\n",
       "             '의미있는': 1,\n",
       "             '결론을': 1,\n",
       "             '내리지': 1,\n",
       "             '못했다.': 1,\n",
       "             '문': 3,\n",
       "             '의장과': 1,\n",
       "             '이인영': 3,\n",
       "             '더불어민주당,': 1,\n",
       "             '나경원': 3,\n",
       "             '자유한국당,': 2,\n",
       "             '오신환': 3,\n",
       "             '바른미래당': 3,\n",
       "             '원내대표는': 7,\n",
       "             '이날': 1,\n",
       "             '오후': 2,\n",
       "             '국회의장실에서': 1,\n",
       "             '회동을': 3,\n",
       "             '갖고': 3,\n",
       "             '국회정상화': 1,\n",
       "             '방안을': 1,\n",
       "             '논의했으나': 1,\n",
       "             '의사일정을': 1,\n",
       "             '잡는데': 1,\n",
       "             '실패했다.': 1,\n",
       "             '의장은': 1,\n",
       "             '이': 2,\n",
       "             '자리에서': 1,\n",
       "             '원내대표에게': 1,\n",
       "             '조속한': 1,\n",
       "             '시일': 1,\n",
       "             '내에': 1,\n",
       "             '국회정상화에': 3,\n",
       "             '합의할': 2,\n",
       "             '것을': 1,\n",
       "             '요청한': 2,\n",
       "             '것으로': 2,\n",
       "             '알려졌다.': 1,\n",
       "             '전날': 1,\n",
       "             '한국당을': 1,\n",
       "             '제외한': 1,\n",
       "             '바른미래당,': 1,\n",
       "             '민주평화당,': 1,\n",
       "             '정의당': 1,\n",
       "             '등': 1,\n",
       "             '야3당과': 1,\n",
       "             '민주당': 3,\n",
       "             '소속': 1,\n",
       "             '의원들을': 1,\n",
       "             '합해': 1,\n",
       "             '98명이': 1,\n",
       "             '6월': 3,\n",
       "             '임시국회': 1,\n",
       "             '소집요구서를': 1,\n",
       "             '제출,': 1,\n",
       "             '임시국회가': 1,\n",
       "             '반쪽으로': 1,\n",
       "             '출발하게': 1,\n",
       "             '됐지만,': 1,\n",
       "             '한국당이': 2,\n",
       "             '끝내': 1,\n",
       "             '국회로': 1,\n",
       "             '복귀하지': 1,\n",
       "             '않을': 1,\n",
       "             '경우': 2,\n",
       "             '임시국회도': 1,\n",
       "             '난항이': 1,\n",
       "             '예상된다.': 1,\n",
       "             '더욱이': 1,\n",
       "             '추가경정예산안을': 1,\n",
       "             '비롯해': 1,\n",
       "             '윤석열': 2,\n",
       "             '검찰총장': 2,\n",
       "             '후보자': 3,\n",
       "             '인사청문회,': 1,\n",
       "             '김현준': 2,\n",
       "             '국세청장': 3,\n",
       "             '인사청문회등': 1,\n",
       "             '굵직한': 1,\n",
       "             '일정이': 2,\n",
       "             '남아있지만': 1,\n",
       "             '한국당의': 1,\n",
       "             '협조': 1,\n",
       "             '없이는': 1,\n",
       "             '이들': 1,\n",
       "             '제대로': 2,\n",
       "             '진행되기': 1,\n",
       "             '어려워': 1,\n",
       "             '일각에서는': 1,\n",
       "             '임시국회만': 1,\n",
       "             '소집해놓고': 1,\n",
       "             '허송세월': 1,\n",
       "             '하는게': 1,\n",
       "             '아니냐': 1,\n",
       "             '하는': 4,\n",
       "             '우려의': 1,\n",
       "             '목소리가': 1,\n",
       "             '나온다.': 1,\n",
       "             '회동': 1,\n",
       "             '직후': 1,\n",
       "             '기자들과': 3,\n",
       "             '만나': 3,\n",
       "             \"'의사일정에\": 1,\n",
       "             '합의를': 2,\n",
       "             \"했느냐'는\": 1,\n",
       "             '기자들의': 1,\n",
       "             '질문에': 7,\n",
       "             '\"안됐다\"면서': 1,\n",
       "             '\"의장님께서': 1,\n",
       "             '합의하라고': 1,\n",
       "             '말씀을': 1,\n",
       "             '하셨는데': 1,\n",
       "             '이견이': 1,\n",
       "             '아직': 1,\n",
       "             '해소될': 1,\n",
       "             '상황이': 1,\n",
       "             '아니다\"고': 1,\n",
       "             '전했다.': 3,\n",
       "             '이어': 2,\n",
       "             '다음': 1,\n",
       "             '묻는': 2,\n",
       "             '질문에도': 1,\n",
       "             '\"아직은': 1,\n",
       "             '모르겠다\"고': 1,\n",
       "             '말했다.': 4,\n",
       "             '한국당': 1,\n",
       "             '원내대표도': 1,\n",
       "             \"'회동에서\": 1,\n",
       "             '게': 3,\n",
       "             \"있느냐'는\": 1,\n",
       "             '\"오늘은': 1,\n",
       "             '특별히': 1,\n",
       "             '말씀드린': 1,\n",
       "             '없는': 1,\n",
       "             '것': 2,\n",
       "             '같다\"고': 1,\n",
       "             '답했다.': 1,\n",
       "             '특히': 1,\n",
       "             '나': 3,\n",
       "             \"'기획재정위원회에서\": 1,\n",
       "             '후보자에': 2,\n",
       "             '대한': 3,\n",
       "             '인사청문회': 1,\n",
       "             '개최에': 1,\n",
       "             '한': 2,\n",
       "             '것이': 2,\n",
       "             '응하겠다는': 1,\n",
       "             \"이야기냐'는\": 1,\n",
       "             '\"최종적으로': 1,\n",
       "             '것은': 5,\n",
       "             '아닌데': 1,\n",
       "             '일부': 2,\n",
       "             '소통에': 1,\n",
       "             '문제가': 2,\n",
       "             '있었던': 1,\n",
       "             '같다\"며': 1,\n",
       "             '\"청문회': 1,\n",
       "             '부분은': 1,\n",
       "             '적극적으로': 1,\n",
       "             '들여다보고': 1,\n",
       "             '있다\"고': 1,\n",
       "             '그는': 1,\n",
       "             \"'소통의\": 1,\n",
       "             '있다는': 1,\n",
       "             '청문회가': 1,\n",
       "             '확정되지': 1,\n",
       "             '않았다는': 1,\n",
       "             \"것이냐'는\": 3,\n",
       "             '\"네.': 1,\n",
       "             '저도': 1,\n",
       "             '조금': 1,\n",
       "             '더': 1,\n",
       "             '확인해보겠다\"고': 1,\n",
       "             '했다.': 1,\n",
       "             '앞서': 1,\n",
       "             '국회': 1,\n",
       "             '기재위': 1,\n",
       "             '여당': 1,\n",
       "             '간사인': 1,\n",
       "             '김정우': 1,\n",
       "             '의원이': 1,\n",
       "             '자유한국당을': 1,\n",
       "             '포함한': 1,\n",
       "             '간사가': 1,\n",
       "             '합의에': 1,\n",
       "             '김': 1,\n",
       "             '인사청문회를': 1,\n",
       "             '오는': 1,\n",
       "             '26일': 1,\n",
       "             '실시하기로': 1,\n",
       "             '합의했다고': 1,\n",
       "             '밝혔지만': 1,\n",
       "             '원내대표에': 1,\n",
       "             '따르면': 1,\n",
       "             '일정도': 1,\n",
       "             '확정이': 1,\n",
       "             '안된': 1,\n",
       "             '보인다.': 1,\n",
       "             '다만': 1,\n",
       "             \"'국회\": 1,\n",
       "             '소집요구가': 1,\n",
       "             '20일인데': 1,\n",
       "             '그': 2,\n",
       "             '전에': 1,\n",
       "             '의사일정에': 1,\n",
       "             '\"국회를': 1,\n",
       "             '정상국회로': 1,\n",
       "             '만들고,': 1,\n",
       "             '국회가': 2,\n",
       "             '정말': 1,\n",
       "             '대한민국의': 1,\n",
       "             '미래,': 1,\n",
       "             '경제와': 1,\n",
       "             '민생을': 1,\n",
       "             '살리는': 1,\n",
       "             '되기': 1,\n",
       "             '계속': 1,\n",
       "             '노력하겠다\"고': 1,\n",
       "             '원내대표들이': 1,\n",
       "             '서울': 1,\n",
       "             '여의도': 1,\n",
       "             '국회에서': 3,\n",
       "             '있다.': 2,\n",
       "             '왼쪽부터': 1,\n",
       "             '더불어민주당': 1,\n",
       "             '원내대표,': 1,\n",
       "             '의장,': 1,\n",
       "             '원내대표.': 1,\n",
       "             '2019.6.18/뉴스1': 1,\n",
       "             '©': 1,\n",
       "             'News1': 1,\n",
       "             '임세영': 1,\n",
       "             '기자': 1,\n",
       "             '\"집권여당은': 1,\n",
       "             '국정운영에': 1,\n",
       "             '무한': 1,\n",
       "             '책임을': 1,\n",
       "             '지고': 1,\n",
       "             '부분에': 1,\n",
       "             '대해': 2,\n",
       "             '민주당이': 2,\n",
       "             '된': 1,\n",
       "             '대승적': 2,\n",
       "             '결단을': 1,\n",
       "             '내려주길': 1,\n",
       "             '바라고,': 1,\n",
       "             '한국당은': 1,\n",
       "             '조건없는': 1,\n",
       "             '참여하기를': 1,\n",
       "             '바라는': 1,\n",
       "             '마음\"이라고': 1,\n",
       "             '밝혔다.': 1,\n",
       "             '오': 2,\n",
       "             \"'여당의\": 1,\n",
       "             '결단이라고': 1,\n",
       "             '함은': 1,\n",
       "             '경제청문회를': 1,\n",
       "             '받아줘야': 1,\n",
       "             '한다는': 1,\n",
       "             '\"집권여당으로서': 1,\n",
       "             '포용하고': 1,\n",
       "             '양보하는': 1,\n",
       "             '숙명이라고': 1,\n",
       "             '생각한다\"며': 1,\n",
       "             '\"야당이': 1,\n",
       "             '경제문제에': 1,\n",
       "             '문제제기를': 1,\n",
       "             '당연한': 1,\n",
       "             '아니냐.': 1,\n",
       "             '청문회건': 1,\n",
       "             '어떤': 1,\n",
       "             '형태가': 1,\n",
       "             '되건': 1,\n",
       "             '정부를': 1,\n",
       "             '상대로': 1,\n",
       "             '할': 1,\n",
       "             '수': 1,\n",
       "             '있는': 2,\n",
       "             '이야기': 1,\n",
       "             '자체를': 1,\n",
       "             '차단하고': 1,\n",
       "             '못하게': 1,\n",
       "             '올지': 1,\n",
       "             '않다고': 1,\n",
       "             '생각한다\"고': 3,\n",
       "             '위원장인': 1,\n",
       "             '상임위의': 1,\n",
       "             '여당이': 1,\n",
       "             '사회권을': 1,\n",
       "             '발동하는': 1,\n",
       "             '것에': 1,\n",
       "             '바른미래당의': 1,\n",
       "             '동참': 1,\n",
       "             '여부를': 1,\n",
       "             '\"그런': 1,\n",
       "             '행동': 1,\n",
       "             '자체가': 1,\n",
       "             '무책임하다고': 1,\n",
       "             '본다\"며': 1,\n",
       "             '\"그것이': 1,\n",
       "             '과연': 1,\n",
       "             '국회를': 1,\n",
       "             '정상적으로': 1,\n",
       "             '운영하기': 1,\n",
       "             '위한': 2,\n",
       "             '마음을': 1,\n",
       "             '것인지,': 1,\n",
       "             '장기적으로': 1,\n",
       "             '추경을': 1,\n",
       "             '처리하고': 1,\n",
       "             '상임위': 1,\n",
       "             '내에서': 1,\n",
       "             '법안을': 1,\n",
       "             '심사하고': 1,\n",
       "             '국민을': 1,\n",
       "             '국회활동이': 1,\n",
       "             '될': 1,\n",
       "             '것인지에': 1,\n",
       "             '판단을': 1,\n",
       "             '해야': 2,\n",
       "             '한다고': 1,\n",
       "             '인사청문회와': 1,\n",
       "             '관련': 1,\n",
       "             '\"인사청문회는': 1,\n",
       "             '당연히': 1,\n",
       "             '절차에': 1,\n",
       "             '따라': 1,\n",
       "             '것\"이라며': 1,\n",
       "             '\"윤': 1,\n",
       "             '후보자가': 1,\n",
       "             '적임자인지': 1,\n",
       "             '여부는': 1,\n",
       "             '국민의': 1,\n",
       "             '눈높이에': 1,\n",
       "             '맞게': 1,\n",
       "             '합리적인': 1,\n",
       "             '인사청문': 1,\n",
       "             '절차를': 1,\n",
       "             '밟아가는': 1,\n",
       "             '마땅하다고': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = kRdd.flatMap(lambda v: v.split(' ')).map(lambda v: (v,1)).countByKey()\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3feba7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1880, 90993, 110491',\n",
       " '1881, 91954, 100745',\n",
       " '1882, 107850, 113688',\n",
       " '1883, 112321, 104629',\n",
       " '1884, 129022, 114445',\n",
       " '1885, 133055, 107800',\n",
       " '1886, 144535, 110784',\n",
       " '1887, 145982, 101414',\n",
       " '1888, 178627, 120853',\n",
       " '1889, 178366, 110584',\n",
       " '1890, 190377, 111025',\n",
       " '1891, 185482, 101196',\n",
       " '1892, 212346, 122037',\n",
       " '1893, 212906, 112317',\n",
       " '1894, 222922, 115772',\n",
       " '1895, 233630, 117398',\n",
       " '1896, 237920, 119570',\n",
       " '1897, 234202, 112758',\n",
       " '1898, 258770, 122693',\n",
       " '1899, 233023, 106212',\n",
       " '1900, 299828, 150499',\n",
       " '1901, 239348, 106471',\n",
       " '1902, 264077, 122659',\n",
       " '1903, 261973, 119234',\n",
       " '1904, 275371, 128125',\n",
       " '1905, 291629, 132319',\n",
       " '1906, 295303, 133159',\n",
       " '1907, 318566, 146833',\n",
       " '1908, 334313, 154344',\n",
       " '1909, 347228, 163999',\n",
       " '1910, 396501, 194218',\n",
       " '1911, 418299, 225968',\n",
       " '1912, 558103, 429945',\n",
       " '1913, 624518, 512557',\n",
       " '1914, 761548, 654762',\n",
       " '1915, 983874, 848603',\n",
       " '1916, 1044323, 890099',\n",
       " '1917, 1081280, 925511',\n",
       " '1918, 1157647, 1013537',\n",
       " '1919, 1130145, 980149',\n",
       " '1920, 1198283, 1064463',\n",
       " '1921, 1232999, 1101457',\n",
       " '1922, 1200919, 1088287',\n",
       " '1923, 1206330, 1096168',\n",
       " '1924, 1248876, 1132751',\n",
       " '1925, 1217352, 1115958',\n",
       " '1926, 1185304, 1110505',\n",
       " '1927, 1192330, 1126717',\n",
       " '1928, 1153117, 1107518',\n",
       " '1929, 1116422, 1075313',\n",
       " '1930, 1125761, 1097176',\n",
       " '1931, 1064497, 1039127',\n",
       " '1932, 1066994, 1043808',\n",
       " '1933, 1007710, 991127',\n",
       " '1934, 1044039, 1032291',\n",
       " '1935, 1048428, 1040930',\n",
       " '1936, 1040202, 1036974',\n",
       " '1937, 1063872, 1066309',\n",
       " '1938, 1103277, 1108841',\n",
       " '1939, 1096518, 1106544',\n",
       " '1940, 1143325, 1158900',\n",
       " '1941, 1207919, 1227941',\n",
       " '1942, 1350545, 1380774',\n",
       " '1943, 1395082, 1426846',\n",
       " '1944, 1327104, 1362555',\n",
       " '1945, 1307028, 1345610',\n",
       " '1946, 1571184, 1623650',\n",
       " '1947, 1772644, 1829223',\n",
       " '1948, 1697814, 1754469',\n",
       " '1949, 1710825, 1773725',\n",
       " '1950, 1713180, 1790673',\n",
       " '1951, 1800042, 1881080',\n",
       " '1952, 1854698, 1944277',\n",
       " '1953, 1880326, 1969777',\n",
       " '1954, 1941682, 2037374',\n",
       " '1955, 1954664, 2057918',\n",
       " '1956, 2007512, 2113694',\n",
       " '1957, 2044160, 2155866',\n",
       " '1958, 2010884, 2120712',\n",
       " '1959, 2023044, 2133509',\n",
       " '1960, 2022093, 2132717',\n",
       " '1961, 2017316, 2122502',\n",
       " '1962, 1966548, 2068945',\n",
       " '1963, 1927217, 2031755',\n",
       " '1964, 1894594, 1993270',\n",
       " '1965, 1765001, 1861378',\n",
       " '1966, 1691868, 1783964',\n",
       " '1967, 1650764, 1744527',\n",
       " '1968, 1640103, 1738928',\n",
       " '1969, 1686947, 1789732',\n",
       " '1970, 1748147, 1859594',\n",
       " '1971, 1663475, 1769201',\n",
       " '1972, 1521185, 1622666',\n",
       " '1973, 1458139, 1559338',\n",
       " '1974, 1467413, 1573105',\n",
       " '1975, 1457699, 1562207',\n",
       " '1976, 1465096, 1569904',\n",
       " '1977, 1532997, 1643684',\n",
       " '1978, 1531658, 1642250',\n",
       " '1979, 1605051, 1721947',\n",
       " '1980, 1659933, 1783876',\n",
       " '1981, 1667465, 1790907',\n",
       " '1982, 1692678, 1813970',\n",
       " '1983, 1670061, 1791732',\n",
       " '1984, 1682973, 1803830',\n",
       " '1985, 1720036, 1847032',\n",
       " '1986, 1714664, 1840164',\n",
       " '1987, 1737891, 1865662',\n",
       " '1988, 1779839, 1912602',\n",
       " '1989, 1843105, 1999999',\n",
       " '1990, 1897709, 2052543',\n",
       " '1991, 1874434, 2019463',\n",
       " '1992, 1843304, 1996388',\n",
       " '1993, 1808379, 1960397',\n",
       " '1994, 1785074, 1931157',\n",
       " '1995, 1757927, 1902832',\n",
       " '1996, 1752249, 1893378',\n",
       " '1997, 1739806, 1884122',\n",
       " '1998, 1765915, 1910250',\n",
       " '1999, 1772632, 1918809',\n",
       " '2000, 1814601, 1962406',\n",
       " '2001, 1799049, 1941251',\n",
       " '2002, 1795206, 1939815',\n",
       " '2003, 1825359, 1973434',\n",
       " '2004, 1834145, 1982794',\n",
       " '2005, 1845379, 1994841',\n",
       " '2006, 1898463, 2052377',\n",
       " '2007, 1919408, 2072139',\n",
       " '2008, 1887234, 2036289',\n",
       " '2009, 1832925, 1979303',\n",
       " '2010, 1772738, 1913851',\n",
       " '2011, 1753500, 1893230']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd = sc.textFile('data/births.txt')\n",
    "bRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41a678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1880', ' 90993', ' 110491'],\n",
       " ['1881', ' 91954', ' 100745'],\n",
       " ['1882', ' 107850', ' 113688'],\n",
       " ['1883', ' 112321', ' 104629'],\n",
       " ['1884', ' 129022', ' 114445'],\n",
       " ['1885', ' 133055', ' 107800'],\n",
       " ['1886', ' 144535', ' 110784'],\n",
       " ['1887', ' 145982', ' 101414'],\n",
       " ['1888', ' 178627', ' 120853'],\n",
       " ['1889', ' 178366', ' 110584'],\n",
       " ['1890', ' 190377', ' 111025'],\n",
       " ['1891', ' 185482', ' 101196'],\n",
       " ['1892', ' 212346', ' 122037'],\n",
       " ['1893', ' 212906', ' 112317'],\n",
       " ['1894', ' 222922', ' 115772'],\n",
       " ['1895', ' 233630', ' 117398'],\n",
       " ['1896', ' 237920', ' 119570'],\n",
       " ['1897', ' 234202', ' 112758'],\n",
       " ['1898', ' 258770', ' 122693'],\n",
       " ['1899', ' 233023', ' 106212'],\n",
       " ['1900', ' 299828', ' 150499'],\n",
       " ['1901', ' 239348', ' 106471'],\n",
       " ['1902', ' 264077', ' 122659'],\n",
       " ['1903', ' 261973', ' 119234'],\n",
       " ['1904', ' 275371', ' 128125'],\n",
       " ['1905', ' 291629', ' 132319'],\n",
       " ['1906', ' 295303', ' 133159'],\n",
       " ['1907', ' 318566', ' 146833'],\n",
       " ['1908', ' 334313', ' 154344'],\n",
       " ['1909', ' 347228', ' 163999'],\n",
       " ['1910', ' 396501', ' 194218'],\n",
       " ['1911', ' 418299', ' 225968'],\n",
       " ['1912', ' 558103', ' 429945'],\n",
       " ['1913', ' 624518', ' 512557'],\n",
       " ['1914', ' 761548', ' 654762'],\n",
       " ['1915', ' 983874', ' 848603'],\n",
       " ['1916', ' 1044323', ' 890099'],\n",
       " ['1917', ' 1081280', ' 925511'],\n",
       " ['1918', ' 1157647', ' 1013537'],\n",
       " ['1919', ' 1130145', ' 980149'],\n",
       " ['1920', ' 1198283', ' 1064463'],\n",
       " ['1921', ' 1232999', ' 1101457'],\n",
       " ['1922', ' 1200919', ' 1088287'],\n",
       " ['1923', ' 1206330', ' 1096168'],\n",
       " ['1924', ' 1248876', ' 1132751'],\n",
       " ['1925', ' 1217352', ' 1115958'],\n",
       " ['1926', ' 1185304', ' 1110505'],\n",
       " ['1927', ' 1192330', ' 1126717'],\n",
       " ['1928', ' 1153117', ' 1107518'],\n",
       " ['1929', ' 1116422', ' 1075313'],\n",
       " ['1930', ' 1125761', ' 1097176'],\n",
       " ['1931', ' 1064497', ' 1039127'],\n",
       " ['1932', ' 1066994', ' 1043808'],\n",
       " ['1933', ' 1007710', ' 991127'],\n",
       " ['1934', ' 1044039', ' 1032291'],\n",
       " ['1935', ' 1048428', ' 1040930'],\n",
       " ['1936', ' 1040202', ' 1036974'],\n",
       " ['1937', ' 1063872', ' 1066309'],\n",
       " ['1938', ' 1103277', ' 1108841'],\n",
       " ['1939', ' 1096518', ' 1106544'],\n",
       " ['1940', ' 1143325', ' 1158900'],\n",
       " ['1941', ' 1207919', ' 1227941'],\n",
       " ['1942', ' 1350545', ' 1380774'],\n",
       " ['1943', ' 1395082', ' 1426846'],\n",
       " ['1944', ' 1327104', ' 1362555'],\n",
       " ['1945', ' 1307028', ' 1345610'],\n",
       " ['1946', ' 1571184', ' 1623650'],\n",
       " ['1947', ' 1772644', ' 1829223'],\n",
       " ['1948', ' 1697814', ' 1754469'],\n",
       " ['1949', ' 1710825', ' 1773725'],\n",
       " ['1950', ' 1713180', ' 1790673'],\n",
       " ['1951', ' 1800042', ' 1881080'],\n",
       " ['1952', ' 1854698', ' 1944277'],\n",
       " ['1953', ' 1880326', ' 1969777'],\n",
       " ['1954', ' 1941682', ' 2037374'],\n",
       " ['1955', ' 1954664', ' 2057918'],\n",
       " ['1956', ' 2007512', ' 2113694'],\n",
       " ['1957', ' 2044160', ' 2155866'],\n",
       " ['1958', ' 2010884', ' 2120712'],\n",
       " ['1959', ' 2023044', ' 2133509'],\n",
       " ['1960', ' 2022093', ' 2132717'],\n",
       " ['1961', ' 2017316', ' 2122502'],\n",
       " ['1962', ' 1966548', ' 2068945'],\n",
       " ['1963', ' 1927217', ' 2031755'],\n",
       " ['1964', ' 1894594', ' 1993270'],\n",
       " ['1965', ' 1765001', ' 1861378'],\n",
       " ['1966', ' 1691868', ' 1783964'],\n",
       " ['1967', ' 1650764', ' 1744527'],\n",
       " ['1968', ' 1640103', ' 1738928'],\n",
       " ['1969', ' 1686947', ' 1789732'],\n",
       " ['1970', ' 1748147', ' 1859594'],\n",
       " ['1971', ' 1663475', ' 1769201'],\n",
       " ['1972', ' 1521185', ' 1622666'],\n",
       " ['1973', ' 1458139', ' 1559338'],\n",
       " ['1974', ' 1467413', ' 1573105'],\n",
       " ['1975', ' 1457699', ' 1562207'],\n",
       " ['1976', ' 1465096', ' 1569904'],\n",
       " ['1977', ' 1532997', ' 1643684'],\n",
       " ['1978', ' 1531658', ' 1642250'],\n",
       " ['1979', ' 1605051', ' 1721947'],\n",
       " ['1980', ' 1659933', ' 1783876'],\n",
       " ['1981', ' 1667465', ' 1790907'],\n",
       " ['1982', ' 1692678', ' 1813970'],\n",
       " ['1983', ' 1670061', ' 1791732'],\n",
       " ['1984', ' 1682973', ' 1803830'],\n",
       " ['1985', ' 1720036', ' 1847032'],\n",
       " ['1986', ' 1714664', ' 1840164'],\n",
       " ['1987', ' 1737891', ' 1865662'],\n",
       " ['1988', ' 1779839', ' 1912602'],\n",
       " ['1989', ' 1843105', ' 1999999'],\n",
       " ['1990', ' 1897709', ' 2052543'],\n",
       " ['1991', ' 1874434', ' 2019463'],\n",
       " ['1992', ' 1843304', ' 1996388'],\n",
       " ['1993', ' 1808379', ' 1960397'],\n",
       " ['1994', ' 1785074', ' 1931157'],\n",
       " ['1995', ' 1757927', ' 1902832'],\n",
       " ['1996', ' 1752249', ' 1893378'],\n",
       " ['1997', ' 1739806', ' 1884122'],\n",
       " ['1998', ' 1765915', ' 1910250'],\n",
       " ['1999', ' 1772632', ' 1918809'],\n",
       " ['2000', ' 1814601', ' 1962406'],\n",
       " ['2001', ' 1799049', ' 1941251'],\n",
       " ['2002', ' 1795206', ' 1939815'],\n",
       " ['2003', ' 1825359', ' 1973434'],\n",
       " ['2004', ' 1834145', ' 1982794'],\n",
       " ['2005', ' 1845379', ' 1994841'],\n",
       " ['2006', ' 1898463', ' 2052377'],\n",
       " ['2007', ' 1919408', ' 2072139'],\n",
       " ['2008', ' 1887234', ' 2036289'],\n",
       " ['2009', ' 1832925', ' 1979303'],\n",
       " ['2010', ' 1772738', ' 1913851'],\n",
       " ['2011', ' 1753500', ' 1893230']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd2 = bRdd.map(lambda v:v.split(',') )\n",
    "bRdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d2611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161802441.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd3 = bRdd2.map(lambda v: (v[1]) ).map(lambda v: float(v)).sum()\n",
    "bRdd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e47bdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225776.0681818184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd4 = bRdd2.map(lambda v: (v[1]) ).map(lambda v: float(v)).mean()\n",
    "bRdd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e45acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164392696.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd5 = bRdd2.map(lambda v: (v[2]) ).map(lambda v: float(v)).sum()\n",
    "bRdd5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7e6800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245399.2121212124"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd6 = bRdd2.map(lambda v: (v[2]) ).map(lambda v: float(v)).mean()\n",
    "bRdd6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "262eb3fb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32, DESKTOP-LOTJ57N, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 730, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 466, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-39-cf1ba1559889>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 730, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 466, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-39-cf1ba1559889>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-cf1ba1559889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbRdd2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1118\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32, DESKTOP-LOTJ57N, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 730, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 466, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-39-cf1ba1559889>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 730, in sortPartition\n    return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 466, in sorted\n    chunk = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-39-cf1ba1559889>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "bRdd2.map(lambda v : v.split(',') ).map(lambda v: int(v[1])).sortBy(lambda v:v ,ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc445818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.60295928629985"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd8 = bRdd2.map(lambda v: (v[1]) ).map(lambda v: float(v)).sum() / ( bRdd2.map(lambda v: (v[1]) ).map(lambda v: float(v)).sum() + bRdd2.map(lambda v: (v[2]) ).map(lambda v: float(v)).sum() ) * 100\n",
    "bRdd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4b827e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1881', ' 91954', ' 100745'],\n",
       " ['1891', ' 185482', ' 101196'],\n",
       " ['1918', ' 1157647', ' 1013537'],\n",
       " ['1887', ' 145982', ' 101414'],\n",
       " ['1934', ' 1044039', ' 1032291'],\n",
       " ['1936', ' 1040202', ' 1036974'],\n",
       " ['1931', ' 1064497', ' 1039127'],\n",
       " ['1935', ' 1048428', ' 1040930'],\n",
       " ['1932', ' 1066994', ' 1043808'],\n",
       " ['1883', ' 112321', ' 104629'],\n",
       " ['1899', ' 233023', ' 106212'],\n",
       " ['1920', ' 1198283', ' 1064463'],\n",
       " ['1901', ' 239348', ' 106471'],\n",
       " ['1937', ' 1063872', ' 1066309'],\n",
       " ['1929', ' 1116422', ' 1075313'],\n",
       " ['1885', ' 133055', ' 107800'],\n",
       " ['1922', ' 1200919', ' 1088287'],\n",
       " ['1923', ' 1206330', ' 1096168'],\n",
       " ['1930', ' 1125761', ' 1097176'],\n",
       " ['1921', ' 1232999', ' 1101457'],\n",
       " ['1880', ' 90993', ' 110491'],\n",
       " ['1889', ' 178366', ' 110584'],\n",
       " ['1939', ' 1096518', ' 1106544'],\n",
       " ['1928', ' 1153117', ' 1107518'],\n",
       " ['1886', ' 144535', ' 110784'],\n",
       " ['1938', ' 1103277', ' 1108841'],\n",
       " ['1890', ' 190377', ' 111025'],\n",
       " ['1926', ' 1185304', ' 1110505'],\n",
       " ['1925', ' 1217352', ' 1115958'],\n",
       " ['1893', ' 212906', ' 112317'],\n",
       " ['1927', ' 1192330', ' 1126717'],\n",
       " ['1897', ' 234202', ' 112758'],\n",
       " ['1924', ' 1248876', ' 1132751'],\n",
       " ['1882', ' 107850', ' 113688'],\n",
       " ['1884', ' 129022', ' 114445'],\n",
       " ['1894', ' 222922', ' 115772'],\n",
       " ['1940', ' 1143325', ' 1158900'],\n",
       " ['1895', ' 233630', ' 117398'],\n",
       " ['1903', ' 261973', ' 119234'],\n",
       " ['1896', ' 237920', ' 119570'],\n",
       " ['1888', ' 178627', ' 120853'],\n",
       " ['1892', ' 212346', ' 122037'],\n",
       " ['1902', ' 264077', ' 122659'],\n",
       " ['1898', ' 258770', ' 122693'],\n",
       " ['1941', ' 1207919', ' 1227941'],\n",
       " ['1904', ' 275371', ' 128125'],\n",
       " ['1905', ' 291629', ' 132319'],\n",
       " ['1906', ' 295303', ' 133159'],\n",
       " ['1945', ' 1307028', ' 1345610'],\n",
       " ['1944', ' 1327104', ' 1362555'],\n",
       " ['1942', ' 1350545', ' 1380774'],\n",
       " ['1943', ' 1395082', ' 1426846'],\n",
       " ['1907', ' 318566', ' 146833'],\n",
       " ['1900', ' 299828', ' 150499'],\n",
       " ['1908', ' 334313', ' 154344'],\n",
       " ['1973', ' 1458139', ' 1559338'],\n",
       " ['1975', ' 1457699', ' 1562207'],\n",
       " ['1976', ' 1465096', ' 1569904'],\n",
       " ['1974', ' 1467413', ' 1573105'],\n",
       " ['1972', ' 1521185', ' 1622666'],\n",
       " ['1946', ' 1571184', ' 1623650'],\n",
       " ['1909', ' 347228', ' 163999'],\n",
       " ['1978', ' 1531658', ' 1642250'],\n",
       " ['1977', ' 1532997', ' 1643684'],\n",
       " ['1979', ' 1605051', ' 1721947'],\n",
       " ['1968', ' 1640103', ' 1738928'],\n",
       " ['1967', ' 1650764', ' 1744527'],\n",
       " ['1948', ' 1697814', ' 1754469'],\n",
       " ['1971', ' 1663475', ' 1769201'],\n",
       " ['1949', ' 1710825', ' 1773725'],\n",
       " ['1980', ' 1659933', ' 1783876'],\n",
       " ['1966', ' 1691868', ' 1783964'],\n",
       " ['1969', ' 1686947', ' 1789732'],\n",
       " ['1950', ' 1713180', ' 1790673'],\n",
       " ['1981', ' 1667465', ' 1790907'],\n",
       " ['1983', ' 1670061', ' 1791732'],\n",
       " ['1984', ' 1682973', ' 1803830'],\n",
       " ['1982', ' 1692678', ' 1813970'],\n",
       " ['1947', ' 1772644', ' 1829223'],\n",
       " ['1986', ' 1714664', ' 1840164'],\n",
       " ['1985', ' 1720036', ' 1847032'],\n",
       " ['1970', ' 1748147', ' 1859594'],\n",
       " ['1965', ' 1765001', ' 1861378'],\n",
       " ['1987', ' 1737891', ' 1865662'],\n",
       " ['1951', ' 1800042', ' 1881080'],\n",
       " ['1997', ' 1739806', ' 1884122'],\n",
       " ['2011', ' 1753500', ' 1893230'],\n",
       " ['1996', ' 1752249', ' 1893378'],\n",
       " ['1995', ' 1757927', ' 1902832'],\n",
       " ['1998', ' 1765915', ' 1910250'],\n",
       " ['1988', ' 1779839', ' 1912602'],\n",
       " ['2010', ' 1772738', ' 1913851'],\n",
       " ['1999', ' 1772632', ' 1918809'],\n",
       " ['1994', ' 1785074', ' 1931157'],\n",
       " ['2002', ' 1795206', ' 1939815'],\n",
       " ['2001', ' 1799049', ' 1941251'],\n",
       " ['1910', ' 396501', ' 194218'],\n",
       " ['1952', ' 1854698', ' 1944277'],\n",
       " ['1993', ' 1808379', ' 1960397'],\n",
       " ['2000', ' 1814601', ' 1962406'],\n",
       " ['1953', ' 1880326', ' 1969777'],\n",
       " ['2003', ' 1825359', ' 1973434'],\n",
       " ['2009', ' 1832925', ' 1979303'],\n",
       " ['2004', ' 1834145', ' 1982794'],\n",
       " ['1964', ' 1894594', ' 1993270'],\n",
       " ['2005', ' 1845379', ' 1994841'],\n",
       " ['1992', ' 1843304', ' 1996388'],\n",
       " ['1989', ' 1843105', ' 1999999'],\n",
       " ['1991', ' 1874434', ' 2019463'],\n",
       " ['1963', ' 1927217', ' 2031755'],\n",
       " ['2008', ' 1887234', ' 2036289'],\n",
       " ['1954', ' 1941682', ' 2037374'],\n",
       " ['2006', ' 1898463', ' 2052377'],\n",
       " ['1990', ' 1897709', ' 2052543'],\n",
       " ['1955', ' 1954664', ' 2057918'],\n",
       " ['1962', ' 1966548', ' 2068945'],\n",
       " ['2007', ' 1919408', ' 2072139'],\n",
       " ['1956', ' 2007512', ' 2113694'],\n",
       " ['1958', ' 2010884', ' 2120712'],\n",
       " ['1961', ' 2017316', ' 2122502'],\n",
       " ['1960', ' 2022093', ' 2132717'],\n",
       " ['1959', ' 2023044', ' 2133509'],\n",
       " ['1957', ' 2044160', ' 2155866'],\n",
       " ['1911', ' 418299', ' 225968'],\n",
       " ['1912', ' 558103', ' 429945'],\n",
       " ['1913', ' 624518', ' 512557'],\n",
       " ['1914', ' 761548', ' 654762'],\n",
       " ['1915', ' 983874', ' 848603'],\n",
       " ['1916', ' 1044323', ' 890099'],\n",
       " ['1917', ' 1081280', ' 925511'],\n",
       " ['1919', ' 1130145', ' 980149'],\n",
       " ['1933', ' 1007710', ' 991127']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRdd9 = bRdd2.sortBy(lambda v:v[2]).collect()\n",
    "bRdd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "280f7c24",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33, DESKTOP-LOTJ57N, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 925, in func\n    initial = next(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-13c2a4fa7746>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 925, in func\n    initial = next(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-13c2a4fa7746>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-13c2a4fa7746>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbRdd2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(self, buckets)\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m                 \u001b[0mminv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\" empty \"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33, DESKTOP-LOTJ57N, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 925, in func\n    initial = next(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-13c2a4fa7746>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 925, in func\n    initial = next(iterator)\n  File \"C:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-40-13c2a4fa7746>\", line 1, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "bRdd2.map(lambda v : v.split(',') ).map(lambda v: int(v[2])).histogram(10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "373160e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
