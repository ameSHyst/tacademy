{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1521eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593a4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array( [[0,0],[0,1],[1,0],[1,1]] )\n",
    "y_data = np.array( [[0],[1],[1],[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a85a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi = LogisticRegression(max_iter = 1000, verbose=1)\n",
    "model_logi.fit(x_data, y_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c3a8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5860f90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae148a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afbc9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logi.score(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce8b21",
   "metadata": {},
   "source": [
    "#### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dffb27b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69912366\n",
      "Iteration 2, loss = 0.69748200\n",
      "Iteration 3, loss = 0.69586784\n",
      "Iteration 4, loss = 0.69441939\n",
      "Iteration 5, loss = 0.69307968\n",
      "Iteration 6, loss = 0.69180842\n",
      "Iteration 7, loss = 0.69063688\n",
      "Iteration 8, loss = 0.68956755\n",
      "Iteration 9, loss = 0.68851915\n",
      "Iteration 10, loss = 0.68752689\n",
      "Iteration 11, loss = 0.68658168\n",
      "Iteration 12, loss = 0.68559858\n",
      "Iteration 13, loss = 0.68462050\n",
      "Iteration 14, loss = 0.68368460\n",
      "Iteration 15, loss = 0.68274096\n",
      "Iteration 16, loss = 0.68179570\n",
      "Iteration 17, loss = 0.68084706\n",
      "Iteration 18, loss = 0.67989924\n",
      "Iteration 19, loss = 0.67896627\n",
      "Iteration 20, loss = 0.67803492\n",
      "Iteration 21, loss = 0.67710567\n",
      "Iteration 22, loss = 0.67618721\n",
      "Iteration 23, loss = 0.67531073\n",
      "Iteration 24, loss = 0.67444908\n",
      "Iteration 25, loss = 0.67358982\n",
      "Iteration 26, loss = 0.67272792\n",
      "Iteration 27, loss = 0.67185568\n",
      "Iteration 28, loss = 0.67098429\n",
      "Iteration 29, loss = 0.67010297\n",
      "Iteration 30, loss = 0.66921527\n",
      "Iteration 31, loss = 0.66833342\n",
      "Iteration 32, loss = 0.66744038\n",
      "Iteration 33, loss = 0.66653699\n",
      "Iteration 34, loss = 0.66565319\n",
      "Iteration 35, loss = 0.66474770\n",
      "Iteration 36, loss = 0.66384152\n",
      "Iteration 37, loss = 0.66296554\n",
      "Iteration 38, loss = 0.66205025\n",
      "Iteration 39, loss = 0.66115451\n",
      "Iteration 40, loss = 0.66025223\n",
      "Iteration 41, loss = 0.65935827\n",
      "Iteration 42, loss = 0.65847649\n",
      "Iteration 43, loss = 0.65757315\n",
      "Iteration 44, loss = 0.65670193\n",
      "Iteration 45, loss = 0.65585373\n",
      "Iteration 46, loss = 0.65496249\n",
      "Iteration 47, loss = 0.65407388\n",
      "Iteration 48, loss = 0.65317717\n",
      "Iteration 49, loss = 0.65227764\n",
      "Iteration 50, loss = 0.65136461\n",
      "Iteration 51, loss = 0.65044114\n",
      "Iteration 52, loss = 0.64950394\n",
      "Iteration 53, loss = 0.64858714\n",
      "Iteration 54, loss = 0.64766495\n",
      "Iteration 55, loss = 0.64672197\n",
      "Iteration 56, loss = 0.64576225\n",
      "Iteration 57, loss = 0.64480636\n",
      "Iteration 58, loss = 0.64385114\n",
      "Iteration 59, loss = 0.64290668\n",
      "Iteration 60, loss = 0.64193846\n",
      "Iteration 61, loss = 0.64095827\n",
      "Iteration 62, loss = 0.63996442\n",
      "Iteration 63, loss = 0.63896662\n",
      "Iteration 64, loss = 0.63802707\n",
      "Iteration 65, loss = 0.63703881\n",
      "Iteration 66, loss = 0.63602168\n",
      "Iteration 67, loss = 0.63498624\n",
      "Iteration 68, loss = 0.63396131\n",
      "Iteration 69, loss = 0.63296631\n",
      "Iteration 70, loss = 0.63194316\n",
      "Iteration 71, loss = 0.63090117\n",
      "Iteration 72, loss = 0.62985860\n",
      "Iteration 73, loss = 0.62879289\n",
      "Iteration 74, loss = 0.62771773\n",
      "Iteration 75, loss = 0.62664823\n",
      "Iteration 76, loss = 0.62559969\n",
      "Iteration 77, loss = 0.62449624\n",
      "Iteration 78, loss = 0.62338697\n",
      "Iteration 79, loss = 0.62227310\n",
      "Iteration 80, loss = 0.62115702\n",
      "Iteration 81, loss = 0.62000820\n",
      "Iteration 82, loss = 0.61884980\n",
      "Iteration 83, loss = 0.61768088\n",
      "Iteration 84, loss = 0.61653457\n",
      "Iteration 85, loss = 0.61536771\n",
      "Iteration 86, loss = 0.61419090\n",
      "Iteration 87, loss = 0.61298189\n",
      "Iteration 88, loss = 0.61179047\n",
      "Iteration 89, loss = 0.61061540\n",
      "Iteration 90, loss = 0.60941088\n",
      "Iteration 91, loss = 0.60820125\n",
      "Iteration 92, loss = 0.60698292\n",
      "Iteration 93, loss = 0.60576657\n",
      "Iteration 94, loss = 0.60454454\n",
      "Iteration 95, loss = 0.60333993\n",
      "Iteration 96, loss = 0.60209674\n",
      "Iteration 97, loss = 0.60086155\n",
      "Iteration 98, loss = 0.59960081\n",
      "Iteration 99, loss = 0.59833198\n",
      "Iteration 100, loss = 0.59709476\n",
      "Iteration 101, loss = 0.59583540\n",
      "Iteration 102, loss = 0.59452195\n",
      "Iteration 103, loss = 0.59323552\n",
      "Iteration 104, loss = 0.59192022\n",
      "Iteration 105, loss = 0.59064878\n",
      "Iteration 106, loss = 0.58933879\n",
      "Iteration 107, loss = 0.58800225\n",
      "Iteration 108, loss = 0.58667328\n",
      "Iteration 109, loss = 0.58533359\n",
      "Iteration 110, loss = 0.58401972\n",
      "Iteration 111, loss = 0.58269726\n",
      "Iteration 112, loss = 0.58136586\n",
      "Iteration 113, loss = 0.57997673\n",
      "Iteration 114, loss = 0.57861557\n",
      "Iteration 115, loss = 0.57725185\n",
      "Iteration 116, loss = 0.57587913\n",
      "Iteration 117, loss = 0.57448564\n",
      "Iteration 118, loss = 0.57309435\n",
      "Iteration 119, loss = 0.57173185\n",
      "Iteration 120, loss = 0.57037151\n",
      "Iteration 121, loss = 0.56898872\n",
      "Iteration 122, loss = 0.56760422\n",
      "Iteration 123, loss = 0.56621296\n",
      "Iteration 124, loss = 0.56480710\n",
      "Iteration 125, loss = 0.56340152\n",
      "Iteration 126, loss = 0.56195367\n",
      "Iteration 127, loss = 0.56054997\n",
      "Iteration 128, loss = 0.55915662\n",
      "Iteration 129, loss = 0.55771577\n",
      "Iteration 130, loss = 0.55625563\n",
      "Iteration 131, loss = 0.55480497\n",
      "Iteration 132, loss = 0.55338181\n",
      "Iteration 133, loss = 0.55188006\n",
      "Iteration 134, loss = 0.55040342\n",
      "Iteration 135, loss = 0.54895690\n",
      "Iteration 136, loss = 0.54750704\n",
      "Iteration 137, loss = 0.54601952\n",
      "Iteration 138, loss = 0.54449336\n",
      "Iteration 139, loss = 0.54299835\n",
      "Iteration 140, loss = 0.54149869\n",
      "Iteration 141, loss = 0.54001814\n",
      "Iteration 142, loss = 0.53851826\n",
      "Iteration 143, loss = 0.53698554\n",
      "Iteration 144, loss = 0.53544089\n",
      "Iteration 145, loss = 0.53392141\n",
      "Iteration 146, loss = 0.53240686\n",
      "Iteration 147, loss = 0.53089100\n",
      "Iteration 148, loss = 0.52937454\n",
      "Iteration 149, loss = 0.52785614\n",
      "Iteration 150, loss = 0.52633503\n",
      "Iteration 151, loss = 0.52479771\n",
      "Iteration 152, loss = 0.52324101\n",
      "Iteration 153, loss = 0.52170147\n",
      "Iteration 154, loss = 0.52012849\n",
      "Iteration 155, loss = 0.51854412\n",
      "Iteration 156, loss = 0.51699439\n",
      "Iteration 157, loss = 0.51543911\n",
      "Iteration 158, loss = 0.51385901\n",
      "Iteration 159, loss = 0.51230387\n",
      "Iteration 160, loss = 0.51077108\n",
      "Iteration 161, loss = 0.50921116\n",
      "Iteration 162, loss = 0.50762313\n",
      "Iteration 163, loss = 0.50607158\n",
      "Iteration 164, loss = 0.50448963\n",
      "Iteration 165, loss = 0.50288820\n",
      "Iteration 166, loss = 0.50128502\n",
      "Iteration 167, loss = 0.49972515\n",
      "Iteration 168, loss = 0.49814318\n",
      "Iteration 169, loss = 0.49654578\n",
      "Iteration 170, loss = 0.49496342\n",
      "Iteration 171, loss = 0.49338983\n",
      "Iteration 172, loss = 0.49178459\n",
      "Iteration 173, loss = 0.49020099\n",
      "Iteration 174, loss = 0.48856270\n",
      "Iteration 175, loss = 0.48695172\n",
      "Iteration 176, loss = 0.48537540\n",
      "Iteration 177, loss = 0.48373944\n",
      "Iteration 178, loss = 0.48216282\n",
      "Iteration 179, loss = 0.48054192\n",
      "Iteration 180, loss = 0.47893998\n",
      "Iteration 181, loss = 0.47732651\n",
      "Iteration 182, loss = 0.47574886\n",
      "Iteration 183, loss = 0.47413934\n",
      "Iteration 184, loss = 0.47253513\n",
      "Iteration 185, loss = 0.47092349\n",
      "Iteration 186, loss = 0.46927535\n",
      "Iteration 187, loss = 0.46766893\n",
      "Iteration 188, loss = 0.46610768\n",
      "Iteration 189, loss = 0.46450133\n",
      "Iteration 190, loss = 0.46286387\n",
      "Iteration 191, loss = 0.46126432\n",
      "Iteration 192, loss = 0.45961651\n",
      "Iteration 193, loss = 0.45801233\n",
      "Iteration 194, loss = 0.45640103\n",
      "Iteration 195, loss = 0.45478147\n",
      "Iteration 196, loss = 0.45315026\n",
      "Iteration 197, loss = 0.45152367\n",
      "Iteration 198, loss = 0.44991005\n",
      "Iteration 199, loss = 0.44830852\n",
      "Iteration 200, loss = 0.44669555\n",
      "Iteration 201, loss = 0.44505297\n",
      "Iteration 202, loss = 0.44343740\n",
      "Iteration 203, loss = 0.44186071\n",
      "Iteration 204, loss = 0.44021968\n",
      "Iteration 205, loss = 0.43860683\n",
      "Iteration 206, loss = 0.43699091\n",
      "Iteration 207, loss = 0.43537429\n",
      "Iteration 208, loss = 0.43377673\n",
      "Iteration 209, loss = 0.43217253\n",
      "Iteration 210, loss = 0.43059517\n",
      "Iteration 211, loss = 0.42898172\n",
      "Iteration 212, loss = 0.42739115\n",
      "Iteration 213, loss = 0.42580754\n",
      "Iteration 214, loss = 0.42418704\n",
      "Iteration 215, loss = 0.42259998\n",
      "Iteration 216, loss = 0.42097605\n",
      "Iteration 217, loss = 0.41936703\n",
      "Iteration 218, loss = 0.41778731\n",
      "Iteration 219, loss = 0.41622285\n",
      "Iteration 220, loss = 0.41458776\n",
      "Iteration 221, loss = 0.41296825\n",
      "Iteration 222, loss = 0.41142379\n",
      "Iteration 223, loss = 0.40984995\n",
      "Iteration 224, loss = 0.40825222\n",
      "Iteration 225, loss = 0.40668051\n",
      "Iteration 226, loss = 0.40509156\n",
      "Iteration 227, loss = 0.40348866\n",
      "Iteration 228, loss = 0.40185743\n",
      "Iteration 229, loss = 0.40031987\n",
      "Iteration 230, loss = 0.39875288\n",
      "Iteration 231, loss = 0.39714664\n",
      "Iteration 232, loss = 0.39556877\n",
      "Iteration 233, loss = 0.39401804\n",
      "Iteration 234, loss = 0.39245633\n",
      "Iteration 235, loss = 0.39087684\n",
      "Iteration 236, loss = 0.38931672\n",
      "Iteration 237, loss = 0.38773975\n",
      "Iteration 238, loss = 0.38618591\n",
      "Iteration 239, loss = 0.38463010\n",
      "Iteration 240, loss = 0.38306696\n",
      "Iteration 241, loss = 0.38150787\n",
      "Iteration 242, loss = 0.37998363\n",
      "Iteration 243, loss = 0.37841739\n",
      "Iteration 244, loss = 0.37685102\n",
      "Iteration 245, loss = 0.37533328\n",
      "Iteration 246, loss = 0.37380812\n",
      "Iteration 247, loss = 0.37226967\n",
      "Iteration 248, loss = 0.37073049\n",
      "Iteration 249, loss = 0.36921262\n",
      "Iteration 250, loss = 0.36769559\n",
      "Iteration 251, loss = 0.36619178\n",
      "Iteration 252, loss = 0.36466528\n",
      "Iteration 253, loss = 0.36311548\n",
      "Iteration 254, loss = 0.36159641\n",
      "Iteration 255, loss = 0.36009305\n",
      "Iteration 256, loss = 0.35858219\n",
      "Iteration 257, loss = 0.35706094\n",
      "Iteration 258, loss = 0.35558258\n",
      "Iteration 259, loss = 0.35410757\n",
      "Iteration 260, loss = 0.35259655\n",
      "Iteration 261, loss = 0.35111053\n",
      "Iteration 262, loss = 0.34960525\n",
      "Iteration 263, loss = 0.34811605\n",
      "Iteration 264, loss = 0.34662545\n",
      "Iteration 265, loss = 0.34517408\n",
      "Iteration 266, loss = 0.34369078\n",
      "Iteration 267, loss = 0.34219389\n",
      "Iteration 268, loss = 0.34074225\n",
      "Iteration 269, loss = 0.33928290\n",
      "Iteration 270, loss = 0.33781271\n",
      "Iteration 271, loss = 0.33632329\n",
      "Iteration 272, loss = 0.33488778\n",
      "Iteration 273, loss = 0.33346176\n",
      "Iteration 274, loss = 0.33202284\n",
      "Iteration 275, loss = 0.33057470\n",
      "Iteration 276, loss = 0.32913274\n",
      "Iteration 277, loss = 0.32768401\n",
      "Iteration 278, loss = 0.32622694\n",
      "Iteration 279, loss = 0.32479753\n",
      "Iteration 280, loss = 0.32337007\n",
      "Iteration 281, loss = 0.32195617\n",
      "Iteration 282, loss = 0.32056276\n",
      "Iteration 283, loss = 0.31916589\n",
      "Iteration 284, loss = 0.31772633\n",
      "Iteration 285, loss = 0.31633839\n",
      "Iteration 286, loss = 0.31494772\n",
      "Iteration 287, loss = 0.31357845\n",
      "Iteration 288, loss = 0.31217542\n",
      "Iteration 289, loss = 0.31076211\n",
      "Iteration 290, loss = 0.30938809\n",
      "Iteration 291, loss = 0.30801745\n",
      "Iteration 292, loss = 0.30662457\n",
      "Iteration 293, loss = 0.30525943\n",
      "Iteration 294, loss = 0.30393135\n",
      "Iteration 295, loss = 0.30256171\n",
      "Iteration 296, loss = 0.30116105\n",
      "Iteration 297, loss = 0.29983052\n",
      "Iteration 298, loss = 0.29849518\n",
      "Iteration 299, loss = 0.29713703\n",
      "Iteration 300, loss = 0.29579830\n",
      "Iteration 301, loss = 0.29447187\n",
      "Iteration 302, loss = 0.29313590\n",
      "Iteration 303, loss = 0.29179756\n",
      "Iteration 304, loss = 0.29048640\n",
      "Iteration 305, loss = 0.28918419\n",
      "Iteration 306, loss = 0.28785533\n",
      "Iteration 307, loss = 0.28655352\n",
      "Iteration 308, loss = 0.28528644\n",
      "Iteration 309, loss = 0.28397606\n",
      "Iteration 310, loss = 0.28266110\n",
      "Iteration 311, loss = 0.28140834\n",
      "Iteration 312, loss = 0.28011168\n",
      "Iteration 313, loss = 0.27883126\n",
      "Iteration 314, loss = 0.27757019\n",
      "Iteration 315, loss = 0.27629750\n",
      "Iteration 316, loss = 0.27503249\n",
      "Iteration 317, loss = 0.27376988\n",
      "Iteration 318, loss = 0.27251567\n",
      "Iteration 319, loss = 0.27127081\n",
      "Iteration 320, loss = 0.27003036\n",
      "Iteration 321, loss = 0.26877370\n",
      "Iteration 322, loss = 0.26752358\n",
      "Iteration 323, loss = 0.26630709\n",
      "Iteration 324, loss = 0.26508823\n",
      "Iteration 325, loss = 0.26386016\n",
      "Iteration 326, loss = 0.26262156\n",
      "Iteration 327, loss = 0.26139955\n",
      "Iteration 328, loss = 0.26018939\n",
      "Iteration 329, loss = 0.25897654\n",
      "Iteration 330, loss = 0.25778881\n",
      "Iteration 331, loss = 0.25660301\n",
      "Iteration 332, loss = 0.25542163\n",
      "Iteration 333, loss = 0.25423945\n",
      "Iteration 334, loss = 0.25304951\n",
      "Iteration 335, loss = 0.25190176\n",
      "Iteration 336, loss = 0.25074831\n",
      "Iteration 337, loss = 0.24956958\n",
      "Iteration 338, loss = 0.24839899\n",
      "Iteration 339, loss = 0.24724001\n",
      "Iteration 340, loss = 0.24608896\n",
      "Iteration 341, loss = 0.24495219\n",
      "Iteration 342, loss = 0.24381311\n",
      "Iteration 343, loss = 0.24266344\n",
      "Iteration 344, loss = 0.24152638\n",
      "Iteration 345, loss = 0.24042022\n",
      "Iteration 346, loss = 0.23929882\n",
      "Iteration 347, loss = 0.23817847\n",
      "Iteration 348, loss = 0.23705395\n",
      "Iteration 349, loss = 0.23596959\n",
      "Iteration 350, loss = 0.23487523\n",
      "Iteration 351, loss = 0.23377448\n",
      "Iteration 352, loss = 0.23268618\n",
      "Iteration 353, loss = 0.23158378\n",
      "Iteration 354, loss = 0.23051094\n",
      "Iteration 355, loss = 0.22942267\n",
      "Iteration 356, loss = 0.22832971\n",
      "Iteration 357, loss = 0.22727046\n",
      "Iteration 358, loss = 0.22620116\n",
      "Iteration 359, loss = 0.22512263\n",
      "Iteration 360, loss = 0.22407494\n",
      "Iteration 361, loss = 0.22303598\n",
      "Iteration 362, loss = 0.22197774\n",
      "Iteration 363, loss = 0.22094682\n",
      "Iteration 364, loss = 0.21992513\n",
      "Iteration 365, loss = 0.21887060\n",
      "Iteration 366, loss = 0.21779691\n",
      "Iteration 367, loss = 0.21673531\n",
      "Iteration 368, loss = 0.21568545\n",
      "Iteration 369, loss = 0.21463296\n",
      "Iteration 370, loss = 0.21363190\n",
      "Iteration 371, loss = 0.21257270\n",
      "Iteration 372, loss = 0.21151380\n",
      "Iteration 373, loss = 0.21048738\n",
      "Iteration 374, loss = 0.20946561\n",
      "Iteration 375, loss = 0.20842724\n",
      "Iteration 376, loss = 0.20737815\n",
      "Iteration 377, loss = 0.20636822\n",
      "Iteration 378, loss = 0.20537463\n",
      "Iteration 379, loss = 0.20435632\n",
      "Iteration 380, loss = 0.20332290\n",
      "Iteration 381, loss = 0.20233161\n",
      "Iteration 382, loss = 0.20133392\n",
      "Iteration 383, loss = 0.20033209\n",
      "Iteration 384, loss = 0.19934037\n",
      "Iteration 385, loss = 0.19834610\n",
      "Iteration 386, loss = 0.19734559\n",
      "Iteration 387, loss = 0.19635637\n",
      "Iteration 388, loss = 0.19541666\n",
      "Iteration 389, loss = 0.19443950\n",
      "Iteration 390, loss = 0.19349547\n",
      "Iteration 391, loss = 0.19255121\n",
      "Iteration 392, loss = 0.19161151\n",
      "Iteration 393, loss = 0.19064222\n",
      "Iteration 394, loss = 0.18966298\n",
      "Iteration 395, loss = 0.18871485\n",
      "Iteration 396, loss = 0.18777685\n",
      "Iteration 397, loss = 0.18685684\n",
      "Iteration 398, loss = 0.18593130\n",
      "Iteration 399, loss = 0.18498360\n",
      "Iteration 400, loss = 0.18408925\n",
      "Iteration 401, loss = 0.18318350\n",
      "Iteration 402, loss = 0.18227023\n",
      "Iteration 403, loss = 0.18135739\n",
      "Iteration 404, loss = 0.18043901\n",
      "Iteration 405, loss = 0.17955396\n",
      "Iteration 406, loss = 0.17865385\n",
      "Iteration 407, loss = 0.17775641\n",
      "Iteration 408, loss = 0.17691595\n",
      "Iteration 409, loss = 0.17604911\n",
      "Iteration 410, loss = 0.17517797\n",
      "Iteration 411, loss = 0.17429436\n",
      "Iteration 412, loss = 0.17343482\n",
      "Iteration 413, loss = 0.17258165\n",
      "Iteration 414, loss = 0.17172845\n",
      "Iteration 415, loss = 0.17087931\n",
      "Iteration 416, loss = 0.17004347\n",
      "Iteration 417, loss = 0.16920949\n",
      "Iteration 418, loss = 0.16837630\n",
      "Iteration 419, loss = 0.16755626\n",
      "Iteration 420, loss = 0.16673254\n",
      "Iteration 421, loss = 0.16590310\n",
      "Iteration 422, loss = 0.16509441\n",
      "Iteration 423, loss = 0.16428060\n",
      "Iteration 424, loss = 0.16346646\n",
      "Iteration 425, loss = 0.16264590\n",
      "Iteration 426, loss = 0.16187035\n",
      "Iteration 427, loss = 0.16107674\n",
      "Iteration 428, loss = 0.16028471\n",
      "Iteration 429, loss = 0.15949971\n",
      "Iteration 430, loss = 0.15872161\n",
      "Iteration 431, loss = 0.15795160\n",
      "Iteration 432, loss = 0.15717224\n",
      "Iteration 433, loss = 0.15641072\n",
      "Iteration 434, loss = 0.15565273\n",
      "Iteration 435, loss = 0.15487933\n",
      "Iteration 436, loss = 0.15412897\n",
      "Iteration 437, loss = 0.15340218\n",
      "Iteration 438, loss = 0.15265211\n",
      "Iteration 439, loss = 0.15188138\n",
      "Iteration 440, loss = 0.15115657\n",
      "Iteration 441, loss = 0.15044384\n",
      "Iteration 442, loss = 0.14971582\n",
      "Iteration 443, loss = 0.14897321\n",
      "Iteration 444, loss = 0.14825498\n",
      "Iteration 445, loss = 0.14755007\n",
      "Iteration 446, loss = 0.14682184\n",
      "Iteration 447, loss = 0.14612206\n",
      "Iteration 448, loss = 0.14541299\n",
      "Iteration 449, loss = 0.14470840\n",
      "Iteration 450, loss = 0.14401368\n",
      "Iteration 451, loss = 0.14332368\n",
      "Iteration 452, loss = 0.14262381\n",
      "Iteration 453, loss = 0.14193878\n",
      "Iteration 454, loss = 0.14127077\n",
      "Iteration 455, loss = 0.14058473\n",
      "Iteration 456, loss = 0.13992400\n",
      "Iteration 457, loss = 0.13925521\n",
      "Iteration 458, loss = 0.13858231\n",
      "Iteration 459, loss = 0.13791783\n",
      "Iteration 460, loss = 0.13727817\n",
      "Iteration 461, loss = 0.13662125\n",
      "Iteration 462, loss = 0.13595271\n",
      "Iteration 463, loss = 0.13530865\n",
      "Iteration 464, loss = 0.13466118\n",
      "Iteration 465, loss = 0.13403125\n",
      "Iteration 466, loss = 0.13339414\n",
      "Iteration 467, loss = 0.13275020\n",
      "Iteration 468, loss = 0.13211551\n",
      "Iteration 469, loss = 0.13149653\n",
      "Iteration 470, loss = 0.13087515\n",
      "Iteration 471, loss = 0.13026016\n",
      "Iteration 472, loss = 0.12963865\n",
      "Iteration 473, loss = 0.12902856\n",
      "Iteration 474, loss = 0.12841875\n",
      "Iteration 475, loss = 0.12781080\n",
      "Iteration 476, loss = 0.12720523\n",
      "Iteration 477, loss = 0.12660265\n",
      "Iteration 478, loss = 0.12602233\n",
      "Iteration 479, loss = 0.12543115\n",
      "Iteration 480, loss = 0.12484305\n",
      "Iteration 481, loss = 0.12425526\n",
      "Iteration 482, loss = 0.12368431\n",
      "Iteration 483, loss = 0.12310291\n",
      "Iteration 484, loss = 0.12252995\n",
      "Iteration 485, loss = 0.12194748\n",
      "Iteration 486, loss = 0.12138269\n",
      "Iteration 487, loss = 0.12082189\n",
      "Iteration 488, loss = 0.12027021\n",
      "Iteration 489, loss = 0.11970363\n",
      "Iteration 490, loss = 0.11914402\n",
      "Iteration 491, loss = 0.11858767\n",
      "Iteration 492, loss = 0.11804701\n",
      "Iteration 493, loss = 0.11748805\n",
      "Iteration 494, loss = 0.11695901\n",
      "Iteration 495, loss = 0.11642616\n",
      "Iteration 496, loss = 0.11587970\n",
      "Iteration 497, loss = 0.11533800\n",
      "Iteration 498, loss = 0.11482222\n",
      "Iteration 499, loss = 0.11429755\n",
      "Iteration 500, loss = 0.11376838\n",
      "Iteration 501, loss = 0.11324772\n",
      "Iteration 502, loss = 0.11273346\n",
      "Iteration 503, loss = 0.11221511\n",
      "Iteration 504, loss = 0.11171639\n",
      "Iteration 505, loss = 0.11120754\n",
      "Iteration 506, loss = 0.11069464\n",
      "Iteration 507, loss = 0.11020586\n",
      "Iteration 508, loss = 0.10970503\n",
      "Iteration 509, loss = 0.10919355\n",
      "Iteration 510, loss = 0.10870020\n",
      "Iteration 511, loss = 0.10822395\n",
      "Iteration 512, loss = 0.10773071\n",
      "Iteration 513, loss = 0.10723973\n",
      "Iteration 514, loss = 0.10675439\n",
      "Iteration 515, loss = 0.10628161\n",
      "Iteration 516, loss = 0.10579643\n",
      "Iteration 517, loss = 0.10532556\n",
      "Iteration 518, loss = 0.10485464\n",
      "Iteration 519, loss = 0.10438385\n",
      "Iteration 520, loss = 0.10391606\n",
      "Iteration 521, loss = 0.10345951\n",
      "Iteration 522, loss = 0.10299944\n",
      "Iteration 523, loss = 0.10253170\n",
      "Iteration 524, loss = 0.10207023\n",
      "Iteration 525, loss = 0.10162207\n",
      "Iteration 526, loss = 0.10117172\n",
      "Iteration 527, loss = 0.10072213\n",
      "Iteration 528, loss = 0.10028254\n",
      "Iteration 529, loss = 0.09983574\n",
      "Iteration 530, loss = 0.09936741\n",
      "Iteration 531, loss = 0.09889890\n",
      "Iteration 532, loss = 0.09843851\n",
      "Iteration 533, loss = 0.09798503\n",
      "Iteration 534, loss = 0.09752848\n",
      "Iteration 535, loss = 0.09706781\n",
      "Iteration 536, loss = 0.09660612\n",
      "Iteration 537, loss = 0.09614344\n",
      "Iteration 538, loss = 0.09568104\n",
      "Iteration 539, loss = 0.09522874\n",
      "Iteration 540, loss = 0.09477404\n",
      "Iteration 541, loss = 0.09432477\n",
      "Iteration 542, loss = 0.09386793\n",
      "Iteration 543, loss = 0.09342305\n",
      "Iteration 544, loss = 0.09297338\n",
      "Iteration 545, loss = 0.09252176\n",
      "Iteration 546, loss = 0.09206950\n",
      "Iteration 547, loss = 0.09166358\n",
      "Iteration 548, loss = 0.09126106\n",
      "Iteration 549, loss = 0.09084686\n",
      "Iteration 550, loss = 0.09043093\n",
      "Iteration 551, loss = 0.09001309\n",
      "Iteration 552, loss = 0.08959352\n",
      "Iteration 553, loss = 0.08918107\n",
      "Iteration 554, loss = 0.08877699\n",
      "Iteration 555, loss = 0.08836698\n",
      "Iteration 556, loss = 0.08794588\n",
      "Iteration 557, loss = 0.08753687\n",
      "Iteration 558, loss = 0.08713812\n",
      "Iteration 559, loss = 0.08672837\n",
      "Iteration 560, loss = 0.08632237\n",
      "Iteration 561, loss = 0.08592407\n",
      "Iteration 562, loss = 0.08552582\n",
      "Iteration 563, loss = 0.08512439\n",
      "Iteration 564, loss = 0.08473321\n",
      "Iteration 565, loss = 0.08433874\n",
      "Iteration 566, loss = 0.08395885\n",
      "Iteration 567, loss = 0.08358633\n",
      "Iteration 568, loss = 0.08320180\n",
      "Iteration 569, loss = 0.08282725\n",
      "Iteration 570, loss = 0.08243782\n",
      "Iteration 571, loss = 0.08204867\n",
      "Iteration 572, loss = 0.08167275\n",
      "Iteration 573, loss = 0.08130378\n",
      "Iteration 574, loss = 0.08093078\n",
      "Iteration 575, loss = 0.08056432\n",
      "Iteration 576, loss = 0.08019681\n",
      "Iteration 577, loss = 0.07983483\n",
      "Iteration 578, loss = 0.07947658\n",
      "Iteration 579, loss = 0.07912304\n",
      "Iteration 580, loss = 0.07877072\n",
      "Iteration 581, loss = 0.07842414\n",
      "Iteration 582, loss = 0.07807621\n",
      "Iteration 583, loss = 0.07772909\n",
      "Iteration 584, loss = 0.07737843\n",
      "Iteration 585, loss = 0.07703611\n",
      "Iteration 586, loss = 0.07667842\n",
      "Iteration 587, loss = 0.07634306\n",
      "Iteration 588, loss = 0.07601446\n",
      "Iteration 589, loss = 0.07567113\n",
      "Iteration 590, loss = 0.07533076\n",
      "Iteration 591, loss = 0.07500619\n",
      "Iteration 592, loss = 0.07467924\n",
      "Iteration 593, loss = 0.07435798\n",
      "Iteration 594, loss = 0.07402839\n",
      "Iteration 595, loss = 0.07370246\n",
      "Iteration 596, loss = 0.07338280\n",
      "Iteration 597, loss = 0.07306047\n",
      "Iteration 598, loss = 0.07274013\n",
      "Iteration 599, loss = 0.07242180\n",
      "Iteration 600, loss = 0.07210092\n",
      "Iteration 601, loss = 0.07179183\n",
      "Iteration 602, loss = 0.07148443\n",
      "Iteration 603, loss = 0.07117272\n",
      "Iteration 604, loss = 0.07086488\n",
      "Iteration 605, loss = 0.07055854\n",
      "Iteration 606, loss = 0.07024634\n",
      "Iteration 607, loss = 0.06994275\n",
      "Iteration 608, loss = 0.06962998\n",
      "Iteration 609, loss = 0.06931600\n",
      "Iteration 610, loss = 0.06899690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 611, loss = 0.06867677\n",
      "Iteration 612, loss = 0.06835052\n",
      "Iteration 613, loss = 0.06801845\n",
      "Iteration 614, loss = 0.06770275\n",
      "Iteration 615, loss = 0.06738250\n",
      "Iteration 616, loss = 0.06706015\n",
      "Iteration 617, loss = 0.06674028\n",
      "Iteration 618, loss = 0.06641555\n",
      "Iteration 619, loss = 0.06611814\n",
      "Iteration 620, loss = 0.06582082\n",
      "Iteration 621, loss = 0.06552521\n",
      "Iteration 622, loss = 0.06523375\n",
      "Iteration 623, loss = 0.06493938\n",
      "Iteration 624, loss = 0.06464589\n",
      "Iteration 625, loss = 0.06434657\n",
      "Iteration 626, loss = 0.06406521\n",
      "Iteration 627, loss = 0.06378279\n",
      "Iteration 628, loss = 0.06348911\n",
      "Iteration 629, loss = 0.06319699\n",
      "Iteration 630, loss = 0.06292054\n",
      "Iteration 631, loss = 0.06263776\n",
      "Iteration 632, loss = 0.06235160\n",
      "Iteration 633, loss = 0.06206323\n",
      "Iteration 634, loss = 0.06178761\n",
      "Iteration 635, loss = 0.06151365\n",
      "Iteration 636, loss = 0.06123347\n",
      "Iteration 637, loss = 0.06095513\n",
      "Iteration 638, loss = 0.06067771\n",
      "Iteration 639, loss = 0.06040608\n",
      "Iteration 640, loss = 0.06013369\n",
      "Iteration 641, loss = 0.05985714\n",
      "Iteration 642, loss = 0.05959503\n",
      "Iteration 643, loss = 0.05932743\n",
      "Iteration 644, loss = 0.05905642\n",
      "Iteration 645, loss = 0.05878954\n",
      "Iteration 646, loss = 0.05852532\n",
      "Iteration 647, loss = 0.05826415\n",
      "Iteration 648, loss = 0.05801218\n",
      "Iteration 649, loss = 0.05776060\n",
      "Iteration 650, loss = 0.05751835\n",
      "Iteration 651, loss = 0.05726942\n",
      "Iteration 652, loss = 0.05701549\n",
      "Iteration 653, loss = 0.05676769\n",
      "Iteration 654, loss = 0.05651676\n",
      "Iteration 655, loss = 0.05626876\n",
      "Iteration 656, loss = 0.05602026\n",
      "Iteration 657, loss = 0.05577206\n",
      "Iteration 658, loss = 0.05553109\n",
      "Iteration 659, loss = 0.05529131\n",
      "Iteration 660, loss = 0.05504726\n",
      "Iteration 661, loss = 0.05481702\n",
      "Iteration 662, loss = 0.05458217\n",
      "Iteration 663, loss = 0.05434514\n",
      "Iteration 664, loss = 0.05410645\n",
      "Iteration 665, loss = 0.05386994\n",
      "Iteration 666, loss = 0.05364161\n",
      "Iteration 667, loss = 0.05340922\n",
      "Iteration 668, loss = 0.05317943\n",
      "Iteration 669, loss = 0.05295078\n",
      "Iteration 670, loss = 0.05272892\n",
      "Iteration 671, loss = 0.05250127\n",
      "Iteration 672, loss = 0.05227626\n",
      "Iteration 673, loss = 0.05205508\n",
      "Iteration 674, loss = 0.05183548\n",
      "Iteration 675, loss = 0.05161366\n",
      "Iteration 676, loss = 0.05139337\n",
      "Iteration 677, loss = 0.05117209\n",
      "Iteration 678, loss = 0.05095732\n",
      "Iteration 679, loss = 0.05074584\n",
      "Iteration 680, loss = 0.05053203\n",
      "Iteration 681, loss = 0.05031949\n",
      "Iteration 682, loss = 0.05010971\n",
      "Iteration 683, loss = 0.04990626\n",
      "Iteration 684, loss = 0.04969575\n",
      "Iteration 685, loss = 0.04948868\n",
      "Iteration 686, loss = 0.04928434\n",
      "Iteration 687, loss = 0.04907827\n",
      "Iteration 688, loss = 0.04888036\n",
      "Iteration 689, loss = 0.04867834\n",
      "Iteration 690, loss = 0.04847633\n",
      "Iteration 691, loss = 0.04827673\n",
      "Iteration 692, loss = 0.04807880\n",
      "Iteration 693, loss = 0.04788613\n",
      "Iteration 694, loss = 0.04768701\n",
      "Iteration 695, loss = 0.04749465\n",
      "Iteration 696, loss = 0.04730706\n",
      "Iteration 697, loss = 0.04711201\n",
      "Iteration 698, loss = 0.04691793\n",
      "Iteration 699, loss = 0.04672737\n",
      "Iteration 700, loss = 0.04654178\n",
      "Iteration 701, loss = 0.04635370\n",
      "Iteration 702, loss = 0.04616634\n",
      "Iteration 703, loss = 0.04598452\n",
      "Iteration 704, loss = 0.04580451\n",
      "Iteration 705, loss = 0.04562158\n",
      "Iteration 706, loss = 0.04544003\n",
      "Iteration 707, loss = 0.04525742\n",
      "Iteration 708, loss = 0.04507463\n",
      "Iteration 709, loss = 0.04489786\n",
      "Iteration 710, loss = 0.04471729\n",
      "Iteration 711, loss = 0.04453968\n",
      "Iteration 712, loss = 0.04436555\n",
      "Iteration 713, loss = 0.04418870\n",
      "Iteration 714, loss = 0.04401571\n",
      "Iteration 715, loss = 0.04384188\n",
      "Iteration 716, loss = 0.04366908\n",
      "Iteration 717, loss = 0.04349709\n",
      "Iteration 718, loss = 0.04333072\n",
      "Iteration 719, loss = 0.04315600\n",
      "Iteration 720, loss = 0.04298880\n",
      "Iteration 721, loss = 0.04282378\n",
      "Iteration 722, loss = 0.04265895\n",
      "Iteration 723, loss = 0.04249017\n",
      "Iteration 724, loss = 0.04232431\n",
      "Iteration 725, loss = 0.04216056\n",
      "Iteration 726, loss = 0.04199519\n",
      "Iteration 727, loss = 0.04183413\n",
      "Iteration 728, loss = 0.04167804\n",
      "Iteration 729, loss = 0.04151671\n",
      "Iteration 730, loss = 0.04135868\n",
      "Iteration 731, loss = 0.04119965\n",
      "Iteration 732, loss = 0.04104509\n",
      "Iteration 733, loss = 0.04088873\n",
      "Iteration 734, loss = 0.04073083\n",
      "Iteration 735, loss = 0.04057748\n",
      "Iteration 736, loss = 0.04042499\n",
      "Iteration 737, loss = 0.04026744\n",
      "Iteration 738, loss = 0.04011323\n",
      "Iteration 739, loss = 0.03996627\n",
      "Iteration 740, loss = 0.03981368\n",
      "Iteration 741, loss = 0.03965516\n",
      "Iteration 742, loss = 0.03950293\n",
      "Iteration 743, loss = 0.03935216\n",
      "Iteration 744, loss = 0.03920469\n",
      "Iteration 745, loss = 0.03905865\n",
      "Iteration 746, loss = 0.03890954\n",
      "Iteration 747, loss = 0.03875913\n",
      "Iteration 748, loss = 0.03861218\n",
      "Iteration 749, loss = 0.03846998\n",
      "Iteration 750, loss = 0.03832465\n",
      "Iteration 751, loss = 0.03817875\n",
      "Iteration 752, loss = 0.03803383\n",
      "Iteration 753, loss = 0.03788925\n",
      "Iteration 754, loss = 0.03774763\n",
      "Iteration 755, loss = 0.03760577\n",
      "Iteration 756, loss = 0.03746333\n",
      "Iteration 757, loss = 0.03732298\n",
      "Iteration 758, loss = 0.03718392\n",
      "Iteration 759, loss = 0.03704407\n",
      "Iteration 760, loss = 0.03690545\n",
      "Iteration 761, loss = 0.03676794\n",
      "Iteration 762, loss = 0.03663164\n",
      "Iteration 763, loss = 0.03649398\n",
      "Iteration 764, loss = 0.03635598\n",
      "Iteration 765, loss = 0.03621927\n",
      "Iteration 766, loss = 0.03608325\n",
      "Iteration 767, loss = 0.03595161\n",
      "Iteration 768, loss = 0.03581488\n",
      "Iteration 769, loss = 0.03568134\n",
      "Iteration 770, loss = 0.03554636\n",
      "Iteration 771, loss = 0.03541562\n",
      "Iteration 772, loss = 0.03528348\n",
      "Iteration 773, loss = 0.03514594\n",
      "Iteration 774, loss = 0.03502081\n",
      "Iteration 775, loss = 0.03489223\n",
      "Iteration 776, loss = 0.03475985\n",
      "Iteration 777, loss = 0.03462923\n",
      "Iteration 778, loss = 0.03449911\n",
      "Iteration 779, loss = 0.03436744\n",
      "Iteration 780, loss = 0.03424133\n",
      "Iteration 781, loss = 0.03411417\n",
      "Iteration 782, loss = 0.03398434\n",
      "Iteration 783, loss = 0.03385456\n",
      "Iteration 784, loss = 0.03373130\n",
      "Iteration 785, loss = 0.03360516\n",
      "Iteration 786, loss = 0.03347715\n",
      "Iteration 787, loss = 0.03334826\n",
      "Iteration 788, loss = 0.03322823\n",
      "Iteration 789, loss = 0.03310779\n",
      "Iteration 790, loss = 0.03298279\n",
      "Iteration 791, loss = 0.03285607\n",
      "Iteration 792, loss = 0.03273447\n",
      "Iteration 793, loss = 0.03261667\n",
      "Iteration 794, loss = 0.03249447\n",
      "Iteration 795, loss = 0.03236796\n",
      "Iteration 796, loss = 0.03224841\n",
      "Iteration 797, loss = 0.03213221\n",
      "Iteration 798, loss = 0.03201313\n",
      "Iteration 799, loss = 0.03189116\n",
      "Iteration 800, loss = 0.03177504\n",
      "Iteration 801, loss = 0.03165951\n",
      "Iteration 802, loss = 0.03154206\n",
      "Iteration 803, loss = 0.03142262\n",
      "Iteration 804, loss = 0.03130509\n",
      "Iteration 805, loss = 0.03119285\n",
      "Iteration 806, loss = 0.03107858\n",
      "Iteration 807, loss = 0.03096173\n",
      "Iteration 808, loss = 0.03084649\n",
      "Iteration 809, loss = 0.03073481\n",
      "Iteration 810, loss = 0.03062109\n",
      "Iteration 811, loss = 0.03050866\n",
      "Iteration 812, loss = 0.03039637\n",
      "Iteration 813, loss = 0.03028246\n",
      "Iteration 814, loss = 0.03017228\n",
      "Iteration 815, loss = 0.03006353\n",
      "Iteration 816, loss = 0.02995481\n",
      "Iteration 817, loss = 0.02984334\n",
      "Iteration 818, loss = 0.02973433\n",
      "Iteration 819, loss = 0.02962875\n",
      "Iteration 820, loss = 0.02951895\n",
      "Iteration 821, loss = 0.02940992\n",
      "Iteration 822, loss = 0.02930412\n",
      "Iteration 823, loss = 0.02919656\n",
      "Iteration 824, loss = 0.02908913\n",
      "Iteration 825, loss = 0.02898036\n",
      "Iteration 826, loss = 0.02887675\n",
      "Iteration 827, loss = 0.02877409\n",
      "Iteration 828, loss = 0.02867018\n",
      "Iteration 829, loss = 0.02856520\n",
      "Iteration 830, loss = 0.02846158\n",
      "Iteration 831, loss = 0.02835942\n",
      "Iteration 832, loss = 0.02825736\n",
      "Iteration 833, loss = 0.02815670\n",
      "Iteration 834, loss = 0.02805695\n",
      "Iteration 835, loss = 0.02795534\n",
      "Iteration 836, loss = 0.02785402\n",
      "Iteration 837, loss = 0.02775535\n",
      "Iteration 838, loss = 0.02765458\n",
      "Iteration 839, loss = 0.02755602\n",
      "Iteration 840, loss = 0.02745773\n",
      "Iteration 841, loss = 0.02735867\n",
      "Iteration 842, loss = 0.02726109\n",
      "Iteration 843, loss = 0.02716660\n",
      "Iteration 844, loss = 0.02707028\n",
      "Iteration 845, loss = 0.02697425\n",
      "Iteration 846, loss = 0.02687839\n",
      "Iteration 847, loss = 0.02678201\n",
      "Iteration 848, loss = 0.02668585\n",
      "Iteration 849, loss = 0.02659541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, verbose=True)\n",
    "model_mlp.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6899c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict(x_data) # 4x2  2x100  4x100  100x1  = 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f5f4dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.score(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca6b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322e245a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0aa173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0514b626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.intercepts_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53568dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55cf9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71493195\n",
      "Iteration 2, loss = 0.70746054\n",
      "Iteration 3, loss = 0.70091816\n",
      "Iteration 4, loss = 0.69463393\n",
      "Iteration 5, loss = 0.68869741\n",
      "Iteration 6, loss = 0.68331059\n",
      "Iteration 7, loss = 0.67845085\n",
      "Iteration 8, loss = 0.67373856\n",
      "Iteration 9, loss = 0.66938532\n",
      "Iteration 10, loss = 0.66557643\n",
      "Iteration 11, loss = 0.66198353\n",
      "Iteration 12, loss = 0.65876174\n",
      "Iteration 13, loss = 0.65567094\n",
      "Iteration 14, loss = 0.65252554\n",
      "Iteration 15, loss = 0.64935641\n",
      "Iteration 16, loss = 0.64618697\n",
      "Iteration 17, loss = 0.64287448\n",
      "Iteration 18, loss = 0.63960276\n",
      "Iteration 19, loss = 0.63626499\n",
      "Iteration 20, loss = 0.63286560\n",
      "Iteration 21, loss = 0.62955639\n",
      "Iteration 22, loss = 0.62620694\n",
      "Iteration 23, loss = 0.62324069\n",
      "Iteration 24, loss = 0.62008923\n",
      "Iteration 25, loss = 0.61675746\n",
      "Iteration 26, loss = 0.61327531\n",
      "Iteration 27, loss = 0.60978484\n",
      "Iteration 28, loss = 0.60620663\n",
      "Iteration 29, loss = 0.60255114\n",
      "Iteration 30, loss = 0.59894775\n",
      "Iteration 31, loss = 0.59521357\n",
      "Iteration 32, loss = 0.59130710\n",
      "Iteration 33, loss = 0.58737307\n",
      "Iteration 34, loss = 0.58356008\n",
      "Iteration 35, loss = 0.57974775\n",
      "Iteration 36, loss = 0.57580849\n",
      "Iteration 37, loss = 0.57176225\n",
      "Iteration 38, loss = 0.56751918\n",
      "Iteration 39, loss = 0.56320060\n",
      "Iteration 40, loss = 0.55885396\n",
      "Iteration 41, loss = 0.55453623\n",
      "Iteration 42, loss = 0.55008016\n",
      "Iteration 43, loss = 0.54547133\n",
      "Iteration 44, loss = 0.54067513\n",
      "Iteration 45, loss = 0.53587414\n",
      "Iteration 46, loss = 0.53106594\n",
      "Iteration 47, loss = 0.52610294\n",
      "Iteration 48, loss = 0.52110960\n",
      "Iteration 49, loss = 0.51597834\n",
      "Iteration 50, loss = 0.51061253\n",
      "Iteration 51, loss = 0.50516551\n",
      "Iteration 52, loss = 0.49978709\n",
      "Iteration 53, loss = 0.49431179\n",
      "Iteration 54, loss = 0.48866393\n",
      "Iteration 55, loss = 0.48292272\n",
      "Iteration 56, loss = 0.47724350\n",
      "Iteration 57, loss = 0.47147342\n",
      "Iteration 58, loss = 0.46560566\n",
      "Iteration 59, loss = 0.45960604\n",
      "Iteration 60, loss = 0.45361140\n",
      "Iteration 61, loss = 0.44765132\n",
      "Iteration 62, loss = 0.44155768\n",
      "Iteration 63, loss = 0.43545334\n",
      "Iteration 64, loss = 0.42922207\n",
      "Iteration 65, loss = 0.42283877\n",
      "Iteration 66, loss = 0.41654461\n",
      "Iteration 67, loss = 0.41005591\n",
      "Iteration 68, loss = 0.40367618\n",
      "Iteration 69, loss = 0.39710413\n",
      "Iteration 70, loss = 0.39049099\n",
      "Iteration 71, loss = 0.38398035\n",
      "Iteration 72, loss = 0.37746535\n",
      "Iteration 73, loss = 0.37088055\n",
      "Iteration 74, loss = 0.36435740\n",
      "Iteration 75, loss = 0.35775056\n",
      "Iteration 76, loss = 0.35117943\n",
      "Iteration 77, loss = 0.34460815\n",
      "Iteration 78, loss = 0.33798591\n",
      "Iteration 79, loss = 0.33142917\n",
      "Iteration 80, loss = 0.32483756\n",
      "Iteration 81, loss = 0.31824282\n",
      "Iteration 82, loss = 0.31171784\n",
      "Iteration 83, loss = 0.30514894\n",
      "Iteration 84, loss = 0.29848814\n",
      "Iteration 85, loss = 0.29215246\n",
      "Iteration 86, loss = 0.28570648\n",
      "Iteration 87, loss = 0.27926084\n",
      "Iteration 88, loss = 0.27287559\n",
      "Iteration 89, loss = 0.26661932\n",
      "Iteration 90, loss = 0.26041201\n",
      "Iteration 91, loss = 0.25427770\n",
      "Iteration 92, loss = 0.24821959\n",
      "Iteration 93, loss = 0.24226356\n",
      "Iteration 94, loss = 0.23640214\n",
      "Iteration 95, loss = 0.23055745\n",
      "Iteration 96, loss = 0.22478185\n",
      "Iteration 97, loss = 0.21915706\n",
      "Iteration 98, loss = 0.21354904\n",
      "Iteration 99, loss = 0.20799438\n",
      "Iteration 100, loss = 0.20263753\n",
      "Iteration 101, loss = 0.19745228\n",
      "Iteration 102, loss = 0.19240270\n",
      "Iteration 103, loss = 0.18735163\n",
      "Iteration 104, loss = 0.18238160\n",
      "Iteration 105, loss = 0.17753664\n",
      "Iteration 106, loss = 0.17279777\n",
      "Iteration 107, loss = 0.16814115\n",
      "Iteration 108, loss = 0.16360258\n",
      "Iteration 109, loss = 0.15912345\n",
      "Iteration 110, loss = 0.15475561\n",
      "Iteration 111, loss = 0.15049466\n",
      "Iteration 112, loss = 0.14639058\n",
      "Iteration 113, loss = 0.14237959\n",
      "Iteration 114, loss = 0.13849667\n",
      "Iteration 115, loss = 0.13468432\n",
      "Iteration 116, loss = 0.13093415\n",
      "Iteration 117, loss = 0.12730386\n",
      "Iteration 118, loss = 0.12378092\n",
      "Iteration 119, loss = 0.12036894\n",
      "Iteration 120, loss = 0.11702928\n",
      "Iteration 121, loss = 0.11380021\n",
      "Iteration 122, loss = 0.11068236\n",
      "Iteration 123, loss = 0.10764850\n",
      "Iteration 124, loss = 0.10471096\n",
      "Iteration 125, loss = 0.10183531\n",
      "Iteration 126, loss = 0.09906160\n",
      "Iteration 127, loss = 0.09637016\n",
      "Iteration 128, loss = 0.09376794\n",
      "Iteration 129, loss = 0.09124518\n",
      "Iteration 130, loss = 0.08880350\n",
      "Iteration 131, loss = 0.08646075\n",
      "Iteration 132, loss = 0.08415617\n",
      "Iteration 133, loss = 0.08192132\n",
      "Iteration 134, loss = 0.07978412\n",
      "Iteration 135, loss = 0.07770664\n",
      "Iteration 136, loss = 0.07567575\n",
      "Iteration 137, loss = 0.07373348\n",
      "Iteration 138, loss = 0.07185670\n",
      "Iteration 139, loss = 0.07003254\n",
      "Iteration 140, loss = 0.06826333\n",
      "Iteration 141, loss = 0.06655352\n",
      "Iteration 142, loss = 0.06489641\n",
      "Iteration 143, loss = 0.06329084\n",
      "Iteration 144, loss = 0.06174730\n",
      "Iteration 145, loss = 0.06025398\n",
      "Iteration 146, loss = 0.05882146\n",
      "Iteration 147, loss = 0.05741094\n",
      "Iteration 148, loss = 0.05606279\n",
      "Iteration 149, loss = 0.05475303\n",
      "Iteration 150, loss = 0.05348335\n",
      "Iteration 151, loss = 0.05224775\n",
      "Iteration 152, loss = 0.05106944\n",
      "Iteration 153, loss = 0.04994496\n",
      "Iteration 154, loss = 0.04884091\n",
      "Iteration 155, loss = 0.04775400\n",
      "Iteration 156, loss = 0.04670544\n",
      "Iteration 157, loss = 0.04569288\n",
      "Iteration 158, loss = 0.04472956\n",
      "Iteration 159, loss = 0.04378571\n",
      "Iteration 160, loss = 0.04285879\n",
      "Iteration 161, loss = 0.04196075\n",
      "Iteration 162, loss = 0.04109206\n",
      "Iteration 163, loss = 0.04025652\n",
      "Iteration 164, loss = 0.03945168\n",
      "Iteration 165, loss = 0.03867282\n",
      "Iteration 166, loss = 0.03791097\n",
      "Iteration 167, loss = 0.03716748\n",
      "Iteration 168, loss = 0.03644657\n",
      "Iteration 169, loss = 0.03575047\n",
      "Iteration 170, loss = 0.03508407\n",
      "Iteration 171, loss = 0.03443085\n",
      "Iteration 172, loss = 0.03379125\n",
      "Iteration 173, loss = 0.03316339\n",
      "Iteration 174, loss = 0.03256138\n",
      "Iteration 175, loss = 0.03197381\n",
      "Iteration 176, loss = 0.03140303\n",
      "Iteration 177, loss = 0.03085277\n",
      "Iteration 178, loss = 0.03031401\n",
      "Iteration 179, loss = 0.02978593\n",
      "Iteration 180, loss = 0.02927146\n",
      "Iteration 181, loss = 0.02877474\n",
      "Iteration 182, loss = 0.02829955\n",
      "Iteration 183, loss = 0.02782430\n",
      "Iteration 184, loss = 0.02736086\n",
      "Iteration 185, loss = 0.02691047\n",
      "Iteration 186, loss = 0.02647667\n",
      "Iteration 187, loss = 0.02605216\n",
      "Iteration 188, loss = 0.02563702\n",
      "Iteration 189, loss = 0.02523653\n",
      "Iteration 190, loss = 0.02484621\n",
      "Iteration 191, loss = 0.02446167\n",
      "Iteration 192, loss = 0.02408380\n",
      "Iteration 193, loss = 0.02371927\n",
      "Iteration 194, loss = 0.02336466\n",
      "Iteration 195, loss = 0.02301571\n",
      "Iteration 196, loss = 0.02267431\n",
      "Iteration 197, loss = 0.02234160\n",
      "Iteration 198, loss = 0.02201708\n",
      "Iteration 199, loss = 0.02169961\n",
      "Iteration 200, loss = 0.02139168\n",
      "Iteration 201, loss = 0.02109128\n",
      "Iteration 202, loss = 0.02079682\n",
      "Iteration 203, loss = 0.02050775\n",
      "Iteration 204, loss = 0.02022580\n",
      "Iteration 205, loss = 0.01994940\n",
      "Iteration 206, loss = 0.01967810\n",
      "Iteration 207, loss = 0.01941604\n",
      "Iteration 208, loss = 0.01915991\n",
      "Iteration 209, loss = 0.01890809\n",
      "Iteration 210, loss = 0.01866423\n",
      "Iteration 211, loss = 0.01842199\n",
      "Iteration 212, loss = 0.01818130\n",
      "Iteration 213, loss = 0.01794988\n",
      "Iteration 214, loss = 0.01772609\n",
      "Iteration 215, loss = 0.01750292\n",
      "Iteration 216, loss = 0.01728356\n",
      "Iteration 217, loss = 0.01707356\n",
      "Iteration 218, loss = 0.01686354\n",
      "Iteration 219, loss = 0.01665745\n",
      "Iteration 220, loss = 0.01645951\n",
      "Iteration 221, loss = 0.01626368\n",
      "Iteration 222, loss = 0.01607004\n",
      "Iteration 223, loss = 0.01588063\n",
      "Iteration 224, loss = 0.01569742\n",
      "Iteration 225, loss = 0.01551662\n",
      "Iteration 226, loss = 0.01533860\n",
      "Iteration 227, loss = 0.01516500\n",
      "Iteration 228, loss = 0.01499391\n",
      "Iteration 229, loss = 0.01482490\n",
      "Iteration 230, loss = 0.01466077\n",
      "Iteration 231, loss = 0.01450014\n",
      "Iteration 232, loss = 0.01434151\n",
      "Iteration 233, loss = 0.01418545\n",
      "Iteration 234, loss = 0.01403169\n",
      "Iteration 235, loss = 0.01388133\n",
      "Iteration 236, loss = 0.01373385\n",
      "Iteration 237, loss = 0.01358973\n",
      "Iteration 238, loss = 0.01344838\n",
      "Iteration 239, loss = 0.01330871\n",
      "Iteration 240, loss = 0.01317211\n",
      "Iteration 241, loss = 0.01303799\n",
      "Iteration 242, loss = 0.01290621\n",
      "Iteration 243, loss = 0.01277613\n",
      "Iteration 244, loss = 0.01264863\n",
      "Iteration 245, loss = 0.01252350\n",
      "Iteration 246, loss = 0.01240029\n",
      "Iteration 247, loss = 0.01227802\n",
      "Iteration 248, loss = 0.01215988\n",
      "Iteration 249, loss = 0.01204410\n",
      "Iteration 250, loss = 0.01192822\n",
      "Iteration 251, loss = 0.01181539\n",
      "Iteration 252, loss = 0.01170310\n",
      "Iteration 253, loss = 0.01159329\n",
      "Iteration 254, loss = 0.01148558\n",
      "Iteration 255, loss = 0.01138016\n",
      "Iteration 256, loss = 0.01127562\n",
      "Iteration 257, loss = 0.01117280\n",
      "Iteration 258, loss = 0.01107194\n",
      "Iteration 259, loss = 0.01097290\n",
      "Iteration 260, loss = 0.01087388\n",
      "Iteration 261, loss = 0.01077816\n",
      "Iteration 262, loss = 0.01068345\n",
      "Iteration 263, loss = 0.01058998\n",
      "Iteration 264, loss = 0.01049790\n",
      "Iteration 265, loss = 0.01040819\n",
      "Iteration 266, loss = 0.01031853\n",
      "Iteration 267, loss = 0.01023029\n",
      "Iteration 268, loss = 0.01014393\n",
      "Iteration 269, loss = 0.01005914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1 = MLPClassifier(hidden_layer_sizes=(100,50), max_iter=1000, verbose=True)\n",
    "model_mlp1.fit(x_data, y_data)      #2(특성데이터)  1(라벨)\n",
    "                                    #2x100, 100x50, 50x1\n",
    "                        #4x2  2x100 = 4x100  100x50  = 4x50  50x1  = 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19eabc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_mlp1.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee69622b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ae6fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae32804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp1.coefs_[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44edfa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81543143\n",
      "Iteration 2, loss = 0.80307655\n",
      "Iteration 3, loss = 0.79097363\n",
      "Iteration 4, loss = 0.77957803\n",
      "Iteration 5, loss = 0.76863069\n",
      "Iteration 6, loss = 0.75839369\n",
      "Iteration 7, loss = 0.74864725\n",
      "Iteration 8, loss = 0.73932604\n",
      "Iteration 9, loss = 0.73022102\n",
      "Iteration 10, loss = 0.72156371\n",
      "Iteration 11, loss = 0.71379601\n",
      "Iteration 12, loss = 0.70644006\n",
      "Iteration 13, loss = 0.69950917\n",
      "Iteration 14, loss = 0.69311268\n",
      "Iteration 15, loss = 0.68894581\n",
      "Iteration 16, loss = 0.68498424\n",
      "Iteration 17, loss = 0.68152568\n",
      "Iteration 18, loss = 0.67907090\n",
      "Iteration 19, loss = 0.67628318\n",
      "Iteration 20, loss = 0.67310976\n",
      "Iteration 21, loss = 0.66982360\n",
      "Iteration 22, loss = 0.66643857\n",
      "Iteration 23, loss = 0.66288045\n",
      "Iteration 24, loss = 0.65943643\n",
      "Iteration 25, loss = 0.65574235\n",
      "Iteration 26, loss = 0.65236742\n",
      "Iteration 27, loss = 0.64984736\n",
      "Iteration 28, loss = 0.64719248\n",
      "Iteration 29, loss = 0.64428514\n",
      "Iteration 30, loss = 0.64103385\n",
      "Iteration 31, loss = 0.63766203\n",
      "Iteration 32, loss = 0.63503165\n",
      "Iteration 33, loss = 0.63210496\n",
      "Iteration 34, loss = 0.62912144\n",
      "Iteration 35, loss = 0.62594266\n",
      "Iteration 36, loss = 0.62319593\n",
      "Iteration 37, loss = 0.62139177\n",
      "Iteration 38, loss = 0.61908417\n",
      "Iteration 39, loss = 0.61657930\n",
      "Iteration 40, loss = 0.61374652\n",
      "Iteration 41, loss = 0.61100049\n",
      "Iteration 42, loss = 0.60842464\n",
      "Iteration 43, loss = 0.60564989\n",
      "Iteration 44, loss = 0.60270876\n",
      "Iteration 45, loss = 0.60006398\n",
      "Iteration 46, loss = 0.59725139\n",
      "Iteration 47, loss = 0.59417946\n",
      "Iteration 48, loss = 0.59091196\n",
      "Iteration 49, loss = 0.58794138\n",
      "Iteration 50, loss = 0.58488694\n",
      "Iteration 51, loss = 0.58175744\n",
      "Iteration 52, loss = 0.57846697\n",
      "Iteration 53, loss = 0.57520272\n",
      "Iteration 54, loss = 0.57175427\n",
      "Iteration 55, loss = 0.56843284\n",
      "Iteration 56, loss = 0.56532682\n",
      "Iteration 57, loss = 0.56222897\n",
      "Iteration 58, loss = 0.55882681\n",
      "Iteration 59, loss = 0.55514223\n",
      "Iteration 60, loss = 0.55151594\n",
      "Iteration 61, loss = 0.54790672\n",
      "Iteration 62, loss = 0.54439817\n",
      "Iteration 63, loss = 0.54089692\n",
      "Iteration 64, loss = 0.53727537\n",
      "Iteration 65, loss = 0.53339435\n",
      "Iteration 66, loss = 0.52953830\n",
      "Iteration 67, loss = 0.52570336\n",
      "Iteration 68, loss = 0.52184525\n",
      "Iteration 69, loss = 0.51809832\n",
      "Iteration 70, loss = 0.51432022\n",
      "Iteration 71, loss = 0.51005924\n",
      "Iteration 72, loss = 0.50597683\n",
      "Iteration 73, loss = 0.50173464\n",
      "Iteration 74, loss = 0.49754978\n",
      "Iteration 75, loss = 0.49314205\n",
      "Iteration 76, loss = 0.48872508\n",
      "Iteration 77, loss = 0.48446096\n",
      "Iteration 78, loss = 0.48009073\n",
      "Iteration 79, loss = 0.47580146\n",
      "Iteration 80, loss = 0.47130886\n",
      "Iteration 81, loss = 0.46655850\n",
      "Iteration 82, loss = 0.46207475\n",
      "Iteration 83, loss = 0.45744870\n",
      "Iteration 84, loss = 0.45282079\n",
      "Iteration 85, loss = 0.44824144\n",
      "Iteration 86, loss = 0.44337767\n",
      "Iteration 87, loss = 0.43863336\n",
      "Iteration 88, loss = 0.43387171\n",
      "Iteration 89, loss = 0.42907628\n",
      "Iteration 90, loss = 0.42431892\n",
      "Iteration 91, loss = 0.41959833\n",
      "Iteration 92, loss = 0.41462400\n",
      "Iteration 93, loss = 0.40942179\n",
      "Iteration 94, loss = 0.40475103\n",
      "Iteration 95, loss = 0.40002401\n",
      "Iteration 96, loss = 0.39522687\n",
      "Iteration 97, loss = 0.39015833\n",
      "Iteration 98, loss = 0.38516854\n",
      "Iteration 99, loss = 0.38020982\n",
      "Iteration 100, loss = 0.37525788\n",
      "Iteration 101, loss = 0.37022251\n",
      "Iteration 102, loss = 0.36537277\n",
      "Iteration 103, loss = 0.36033933\n",
      "Iteration 104, loss = 0.35515909\n",
      "Iteration 105, loss = 0.35030731\n",
      "Iteration 106, loss = 0.34534515\n",
      "Iteration 107, loss = 0.34014725\n",
      "Iteration 108, loss = 0.33509037\n",
      "Iteration 109, loss = 0.32993646\n",
      "Iteration 110, loss = 0.32494456\n",
      "Iteration 111, loss = 0.31995414\n",
      "Iteration 112, loss = 0.31500100\n",
      "Iteration 113, loss = 0.31001101\n",
      "Iteration 114, loss = 0.30484419\n",
      "Iteration 115, loss = 0.29995272\n",
      "Iteration 116, loss = 0.29495861\n",
      "Iteration 117, loss = 0.28993010\n",
      "Iteration 118, loss = 0.28487293\n",
      "Iteration 119, loss = 0.28003118\n",
      "Iteration 120, loss = 0.27533947\n",
      "Iteration 121, loss = 0.27039097\n",
      "Iteration 122, loss = 0.26540183\n",
      "Iteration 123, loss = 0.26040210\n",
      "Iteration 124, loss = 0.25572630\n",
      "Iteration 125, loss = 0.25084713\n",
      "Iteration 126, loss = 0.24613997\n",
      "Iteration 127, loss = 0.24146757\n",
      "Iteration 128, loss = 0.23674354\n",
      "Iteration 129, loss = 0.23210175\n",
      "Iteration 130, loss = 0.22737245\n",
      "Iteration 131, loss = 0.22282242\n",
      "Iteration 132, loss = 0.21845854\n",
      "Iteration 133, loss = 0.21394083\n",
      "Iteration 134, loss = 0.20951974\n",
      "Iteration 135, loss = 0.20530874\n",
      "Iteration 136, loss = 0.20102746\n",
      "Iteration 137, loss = 0.19659818\n",
      "Iteration 138, loss = 0.19254281\n",
      "Iteration 139, loss = 0.18845410\n",
      "Iteration 140, loss = 0.18424809\n",
      "Iteration 141, loss = 0.18026676\n",
      "Iteration 142, loss = 0.17617461\n",
      "Iteration 143, loss = 0.17215393\n",
      "Iteration 144, loss = 0.16830327\n",
      "Iteration 145, loss = 0.16437771\n",
      "Iteration 146, loss = 0.16057117\n",
      "Iteration 147, loss = 0.15688422\n",
      "Iteration 148, loss = 0.15330753\n",
      "Iteration 149, loss = 0.14977431\n",
      "Iteration 150, loss = 0.14623678\n",
      "Iteration 151, loss = 0.14269046\n",
      "Iteration 152, loss = 0.13933080\n",
      "Iteration 153, loss = 0.13598005\n",
      "Iteration 154, loss = 0.13263601\n",
      "Iteration 155, loss = 0.12940442\n",
      "Iteration 156, loss = 0.12631961\n",
      "Iteration 157, loss = 0.12324407\n",
      "Iteration 158, loss = 0.12019217\n",
      "Iteration 159, loss = 0.11720697\n",
      "Iteration 160, loss = 0.11443056\n",
      "Iteration 161, loss = 0.11162261\n",
      "Iteration 162, loss = 0.10881503\n",
      "Iteration 163, loss = 0.10618743\n",
      "Iteration 164, loss = 0.10357252\n",
      "Iteration 165, loss = 0.10102492\n",
      "Iteration 166, loss = 0.09855653\n",
      "Iteration 167, loss = 0.09608893\n",
      "Iteration 168, loss = 0.09378173\n",
      "Iteration 169, loss = 0.09150681\n",
      "Iteration 170, loss = 0.08923055\n",
      "Iteration 171, loss = 0.08714624\n",
      "Iteration 172, loss = 0.08501584\n",
      "Iteration 173, loss = 0.08294902\n",
      "Iteration 174, loss = 0.08100772\n",
      "Iteration 175, loss = 0.07898005\n",
      "Iteration 176, loss = 0.07715030\n",
      "Iteration 177, loss = 0.07529383\n",
      "Iteration 178, loss = 0.07347207\n",
      "Iteration 179, loss = 0.07176530\n",
      "Iteration 180, loss = 0.07005741\n",
      "Iteration 181, loss = 0.06837623\n",
      "Iteration 182, loss = 0.06672844\n",
      "Iteration 183, loss = 0.06510091\n",
      "Iteration 184, loss = 0.06353824\n",
      "Iteration 185, loss = 0.06202689\n",
      "Iteration 186, loss = 0.06056437\n",
      "Iteration 187, loss = 0.05911975\n",
      "Iteration 188, loss = 0.05770650\n",
      "Iteration 189, loss = 0.05632539\n",
      "Iteration 190, loss = 0.05500158\n",
      "Iteration 191, loss = 0.05370994\n",
      "Iteration 192, loss = 0.05245546\n",
      "Iteration 193, loss = 0.05122525\n",
      "Iteration 194, loss = 0.05003389\n",
      "Iteration 195, loss = 0.04888900\n",
      "Iteration 196, loss = 0.04780147\n",
      "Iteration 197, loss = 0.04669059\n",
      "Iteration 198, loss = 0.04564931\n",
      "Iteration 199, loss = 0.04464719\n",
      "Iteration 200, loss = 0.04365794\n",
      "Iteration 201, loss = 0.04269733\n",
      "Iteration 202, loss = 0.04177314\n",
      "Iteration 203, loss = 0.04087972\n",
      "Iteration 204, loss = 0.04000537\n",
      "Iteration 205, loss = 0.03916068\n",
      "Iteration 206, loss = 0.03834621\n",
      "Iteration 207, loss = 0.03753366\n",
      "Iteration 208, loss = 0.03675080\n",
      "Iteration 209, loss = 0.03597453\n",
      "Iteration 210, loss = 0.03523311\n",
      "Iteration 211, loss = 0.03452308\n",
      "Iteration 212, loss = 0.03381478\n",
      "Iteration 213, loss = 0.03312382\n",
      "Iteration 214, loss = 0.03246857\n",
      "Iteration 215, loss = 0.03181701\n",
      "Iteration 216, loss = 0.03119764\n",
      "Iteration 217, loss = 0.03059256\n",
      "Iteration 218, loss = 0.02999273\n",
      "Iteration 219, loss = 0.02941724\n",
      "Iteration 220, loss = 0.02885073\n",
      "Iteration 221, loss = 0.02830767\n",
      "Iteration 222, loss = 0.02776878\n",
      "Iteration 223, loss = 0.02725921\n",
      "Iteration 224, loss = 0.02675915\n",
      "Iteration 225, loss = 0.02626912\n",
      "Iteration 226, loss = 0.02579163\n",
      "Iteration 227, loss = 0.02531816\n",
      "Iteration 228, loss = 0.02486800\n",
      "Iteration 229, loss = 0.02442775\n",
      "Iteration 230, loss = 0.02400587\n",
      "Iteration 231, loss = 0.02359971\n",
      "Iteration 232, loss = 0.02319443\n",
      "Iteration 233, loss = 0.02279355\n",
      "Iteration 234, loss = 0.02242260\n",
      "Iteration 235, loss = 0.02203905\n",
      "Iteration 236, loss = 0.02167459\n",
      "Iteration 237, loss = 0.02132176\n",
      "Iteration 238, loss = 0.02096745\n",
      "Iteration 239, loss = 0.02063402\n",
      "Iteration 240, loss = 0.02029892\n",
      "Iteration 241, loss = 0.01998161\n",
      "Iteration 242, loss = 0.01966663\n",
      "Iteration 243, loss = 0.01935669\n",
      "Iteration 244, loss = 0.01906523\n",
      "Iteration 245, loss = 0.01877918\n",
      "Iteration 246, loss = 0.01848949\n",
      "Iteration 247, loss = 0.01820842\n",
      "Iteration 248, loss = 0.01794018\n",
      "Iteration 249, loss = 0.01767528\n",
      "Iteration 250, loss = 0.01741610\n",
      "Iteration 251, loss = 0.01716432\n",
      "Iteration 252, loss = 0.01691612\n",
      "Iteration 253, loss = 0.01667162\n",
      "Iteration 254, loss = 0.01643283\n",
      "Iteration 255, loss = 0.01620467\n",
      "Iteration 256, loss = 0.01597983\n",
      "Iteration 257, loss = 0.01575804\n",
      "Iteration 258, loss = 0.01554533\n",
      "Iteration 259, loss = 0.01533420\n",
      "Iteration 260, loss = 0.01512592\n",
      "Iteration 261, loss = 0.01492229\n",
      "Iteration 262, loss = 0.01472611\n",
      "Iteration 263, loss = 0.01453379\n",
      "Iteration 264, loss = 0.01434518\n",
      "Iteration 265, loss = 0.01415876\n",
      "Iteration 266, loss = 0.01397522\n",
      "Iteration 267, loss = 0.01379621\n",
      "Iteration 268, loss = 0.01362330\n",
      "Iteration 269, loss = 0.01345010\n",
      "Iteration 270, loss = 0.01327994\n",
      "Iteration 271, loss = 0.01311769\n",
      "Iteration 272, loss = 0.01295616\n",
      "Iteration 273, loss = 0.01279559\n",
      "Iteration 274, loss = 0.01263660\n",
      "Iteration 275, loss = 0.01248357\n",
      "Iteration 276, loss = 0.01232899\n",
      "Iteration 277, loss = 0.01217810\n",
      "Iteration 278, loss = 0.01202605\n",
      "Iteration 279, loss = 0.01187714\n",
      "Iteration 280, loss = 0.01173393\n",
      "Iteration 281, loss = 0.01159229\n",
      "Iteration 282, loss = 0.01145015\n",
      "Iteration 283, loss = 0.01131190\n",
      "Iteration 284, loss = 0.01117566\n",
      "Iteration 285, loss = 0.01104142\n",
      "Iteration 286, loss = 0.01090680\n",
      "Iteration 287, loss = 0.01077581\n",
      "Iteration 288, loss = 0.01064865\n",
      "Iteration 289, loss = 0.01052102\n",
      "Iteration 290, loss = 0.01039503\n",
      "Iteration 291, loss = 0.01027319\n",
      "Iteration 292, loss = 0.01015081\n",
      "Iteration 293, loss = 0.01003165\n",
      "Iteration 294, loss = 0.00991298\n",
      "Iteration 295, loss = 0.00979660\n",
      "Iteration 296, loss = 0.00968291\n",
      "Iteration 297, loss = 0.00957190\n",
      "Iteration 298, loss = 0.00945994\n",
      "Iteration 299, loss = 0.00934866\n",
      "Iteration 300, loss = 0.00924037\n",
      "Iteration 301, loss = 0.00913348\n",
      "Iteration 302, loss = 0.00902790\n",
      "Iteration 303, loss = 0.00892063\n",
      "Iteration 304, loss = 0.00881976\n",
      "Iteration 305, loss = 0.00871887\n",
      "Iteration 306, loss = 0.00861504\n",
      "Iteration 307, loss = 0.00851551\n",
      "Iteration 308, loss = 0.00841831\n",
      "Iteration 309, loss = 0.00832178\n",
      "Iteration 310, loss = 0.00822455\n",
      "Iteration 311, loss = 0.00812919\n",
      "Iteration 312, loss = 0.00803748\n",
      "Iteration 313, loss = 0.00794633\n",
      "Iteration 314, loss = 0.00785503\n",
      "Iteration 315, loss = 0.00776372\n",
      "Iteration 316, loss = 0.00767929\n",
      "Iteration 317, loss = 0.00758959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TFG5076XG\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2 = MLPClassifier(hidden_layer_sizes=(64,32,16), max_iter=1000, verbose=True)\n",
    "model_mlp2.fit(x_data, y_data)          #2(특성데이터)  1(라벨)\n",
    "                                    #2x64, 64x32, 32x16, 16x1\n",
    "                        #4x2  2x64 = 4x64  64x32  = 4x32  32x16  = 4x16  16x1  = 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b27c33ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_mlp2.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c2123da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b204ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3167300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e42585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp2.coefs_[3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
